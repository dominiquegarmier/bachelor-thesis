\chapter{Data Clustering}
\label{chapter__dataclustering}

We first give a few examples of clustering algorithms, which are commonly studied in machine learning and data science. In particular, we discuss classical and hierarchical clustering algorithms. For this we will base ourselves on the books \cite{Everitt2011} and \cite{Scitovski2021}.
We will contextualize these algorithms with a few example computations and will see how they can fall short in certain cases.

Then we talk about how clustering algorithms can be seen as remaining invariant under certain transformations of its input. We give some intuition why this might be a desirable property for a clustering algorithm. This will lead to \textsc{Kleinberg}'s impossibility result \cite{Kleinberg2002}.

\source[CM10a as a source for bad things that can happen]

\section{Examples of Clustering Algorithms}
Let us consider a few examples of clustering algorithms.

\subsection{$k$-means}
A common method of clustering is to find a $k$-partition of the points according to some criterion.
One such example is the \emph{$k$-means} clustering objective \cite[Sec.~3.1]{Scitovski2021}.

Let $x_1, \dots, x_n \in \R^d$ be a set of points.

\begin{definition}{$k$-means Clustering}{kmeans_clustering}
For $k \in \N$ the \emph{$k$-means clustering objective} refers to finding a partition $C_1, \dots, C_k$ of $\{x_1, \dots, x_n\}$ such that
\begin{equation*}
\label{eq:kmean_optimization}
    \sum_{i = 1}^k \sum_{x \in C_i} \|x - \mu_i\|^2
\end{equation*}
is minimal, where $\mu_k := \frac{1}{|C_k|}\sum_{x \in C_k} x$ is the \emph{center} of $C_k$\footnote{In case $C_k = \emptyset$ we set $\mu_k = 0 \in \R^d$.}.
\end{definition}
To find such a partition we would have to check all possible partitions of $x_1, \dots, x_n$. This is computationally unfeasible. In practice, we use approximate methods such as \textsc{Lloyd}'s algorithm \cite[Sec.~3.1.2]{Scitovski2021}.
Sometimes it is simply referred to as \emph{the $k$-means algorithm}.

\begin{definition}{Lloyd's Algorithm}{lloyds_algorithm}
Given $\{x_1, \dots, x_n\} \subset \R^d$ and some \emph{centers} $z_1, \dots, z_k \in \R^d$
Lloyd's algorithm consists of iteratively applying the following two steps:

\begin{enumerate}
    \item \textbf{Assignment Step:} Assign each point $x_i$ to the cluster $C_j$ such that
    $$
    j = \argmin_{1 \leq l \leq k} \|x_i - z_l\|^2.
    $$

    \item \textbf{Update Step:} Update the centers $z_1, \dots, z_k$ by setting
    $$
    z_j = \frac{1}{|C_j|}\sum_{x \in C_j} x. \quad \forall \quad j \in \{1, \dots, k\},
    $$
    where $z_j = 0$ if $C_j = \emptyset$. 
\end{enumerate}
\end{definition}

One can show that Lloyd's algorithm converges after finitely many steps \cite[Thm.~3.14]{Scitovski2021}.
We will see that the choice of starting centers $z_1, \dots, z_k$ can have a significant impact on the outcome of the algorithm.

Consider the set $\{(0,0), (0,1), (2,0), (2,1)\} \subset \R^2$ and $k = 2$. And two possible choices of starting centers:

\begin{enumerate}
    \item $z_1 = (0, 1/2)$ and $z_2 = (2, 1/2)$
    \item $z_1' = (1, 0)$ and $z_2' = (1, 1)$
\end{enumerate}

In both cases the algorithm will converge after just one iteration. But depending on the choice of starting centers we will get the clusters seen in figure \ref{fig:kmeans_example}.

%\begin{enumerate}
%    \item $C_1 = \{(0,0), (0,1)\}$ and $C_2 = \{(2,0), (2,1)\}$
%    \item $C_1' = \{(0,0), (2,0)\}$ and $C_2' = \{(0,1), (2,1)\}$
%\end{enumerate}

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
        \node[circle,fill=black,inner sep=1pt] at (0,0) {};
        \node[circle,fill=black,inner sep=1pt] at (0,1.5) {};
        \node[circle,fill=black,inner sep=1pt] at (3,0) {};
        \node[circle,fill=black,inner sep=1pt] at (3,1.5) {};
    
        \node[circle,fill=myblue,inner sep=1pt, label={[myblue]:\small$z_1$}] at (0,0.75) {};
        \node[circle,fill=myred,inner sep=1pt, label={[myred]:\small$z_2$}] at (3,0.75) {};
    
        \draw[ellipse,dashed,fill=myblue,fill opacity=0.15] (0,0.75) ellipse (0.75 and 1.5);
        \draw[ellipse,dashed,fill=myred,fill opacity=0.15] (3,0.75) ellipse (0.75 and 1.5);
    \end{tikzpicture}
    \caption*{starting centers $(1)$}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
        \node[circle,fill=black,inner sep=1pt] at (0,0) {};
        \node[circle,fill=black,inner sep=1pt] at (0,1.5) {};
        \node[circle,fill=black,inner sep=1pt] at (3,0) {};
        \node[circle,fill=black,inner sep=1pt] at (3,1.5) {};
    
        \node[circle,fill=myblue,inner sep=1pt, label={[myblue]:\small$z_1'$}] at (1.5,0) {};
        \node[circle,fill=myred,inner sep=1pt, label={[myred]:\small$z_2'$}] at (1.5,1.5) {};
    
        \draw[ellipse,dashed,fill=myblue,fill opacity=0.15] (1.5,0) ellipse (2 and 0.65);
        \draw[ellipse,dashed,fill=myred,fill opacity=0.15] (1.5,1.5) ellipse (2 and 0.65);
    \end{tikzpicture}
    \caption*{starting centers $(2)$}
\end{subfigure}
\caption{$k$-means clustering for different starting centers}
\label{fig:kmeans_example}
\end{figure}

In fact the first solution is the one minimizing the $k$-means objective. So we see that the choice of initial centers is important for the outcome of the algorithm. For this reason in practice one might run the algorithm multiple times with different initial centers drawn from some distribution and then choose the best solution. \source[]

Another limitation of the $k$-means algorithm is that it's clusters are always convex. Consider the following data points in figure \ref{fig:kemans_circles} for $k = 2$.

\begin{figure*}[h]
\centering
\begin{tikzpicture}
    \draw[circle, dashed](0,0) circle (1);
    \draw[circle, dashed](0,0) circle (2);

    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0095579832468025,-0.23458616178292213) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8621356010904919,-0.43521673725850546) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8627616584531776,0.4128659891205529) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8291421634685034,0.5054463113114217) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9764896823801532,0.12472092624358487) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0140626107241928,-0.13705972885468481) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.7797471398667898,0.7164817919382073) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5877999996419829,0.7418100674775248) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9897424809214725,-0.35865675131466895) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.0922792389809537,1.006191078288321) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9935603794343448,0.11784757153395319) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.026524547115352,-0.37871897925085024) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9488139684656478,0.7747854617416964) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5164088657878309,2.03153994637684) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.429935616806283,1.5022872229353683) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.8365483721220037,1.9189627548136554) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.981301736481553,-0.14499075605710074) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6349515928709943,1.2202599759050379) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5231409868905119,1.9793981704336547) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.053624362266639,1.6908381811818811) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.04231412307509439,2.045672752950578) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.06811082126040592,1.9438370464512291) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0906354601250503,1.694220851300212) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.589017397924315,1.3208705982413913) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.2875236513311277,1.5206616702727054) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.6515674232388111,1.8678604281807427) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6558729051313483,1.1575803869149104) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6685583816277019,1.1508647790179702) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9448141306887845,-0.28977280114742615) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9472636540198844,-0.28636654246930904) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9295785207047487,0.48494891715335603) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.41340373800077374,1.97704090631188) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.416451128927296,1.3334319841181093) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.5504121965152975,1.8450892982198965) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.484969911606966,1.3723065854767136) {};
    % -----------------
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.914573789718945,-0.28964318073223894) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.1982533125570065,-0.8919178733968665) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.80577538812941,-0.607080998055582) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9412027083485228,-0.27269568749492573) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8148270419175926,0.4238878668815002) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6225375984421799,-0.8119486329925646) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8821678647126937,0.3648396878561235) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.7795857969200991,-0.6304341921488942) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7483950763114702,0.6729373613544269) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.2739384002725763,-1.0208542531185714) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9228380796524287,-0.16940862942773213) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9282962576231144,-0.42297153472927584) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.49750583441724194,-0.8706964828847926) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.16573977530367748,-0.9270432989794191) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.7154381602074034,-0.761747732857447) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.28063484315157317,-0.8794997600003289) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.0430965817766624,0.2575111200438646) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.20871850422394092,-1.0747146100323817) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6320684095077692,-0.7632679571994616) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9706230943148069,-0.13408763258353848) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9122031196423117,0.5420940005463377) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.15908376563698895,-1.0366970717997068) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9794313663308976,0.2768904673101241) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9410855486192096,-0.43158246153463864) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8050509365055522,-0.7337938215702184) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7668888938180021,-0.6516232811273315) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7569340532084752,-0.683852150730453) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.3874097599372178,-0.8483389110540501) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.2297367654792433,1.4958896410698033) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.8902258184052567,0.7894776893682616) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6273208428361784,-1.8371947162534354) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9885817783568538,-1.7147623663359197) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9386023582122706,-1.7236389913807584) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.3401106297221774,1.5451099391524472) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.7696595846944951,-0.835412527730738) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.94149620924287,0.7884962462992549) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.1613066182943586,-1.6658708071329087) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.9737616591333986,0.42415122258148036) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.0910291132157077,-1.6346094222929535) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.420125841706075,1.5367643867093461) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.8773373323041873,-1.83463023722728) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (1.0295543643648137,-1.6695857338970421) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.9628887251833387,0.3917097103265355) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.6082435557290202,-1.2449832245663626) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.1367588700508612,-2.0242149878928153) {};

    \draw[dashed, color=myred, line width=1] (-1.9, 2.4) -- (2.2, -2); % diff = (4.1, -4.4)
    \draw[->, color=myred, >=stealth, line width=1] (-1.9 + 0.41, 2.4 - 0.44) -- (-1.9 + 0.41 + 0.22, 2.4 - 0.44 + 0.205);
    \draw[->, color=myred, >=stealth, line width=1] (2.2 - 0.41, -2 + 0.44) -- (2.2 - 0.41 + 0.22, -2 + 0.44 + 0.205);
\end{tikzpicture}
\caption{Points generated by adding noise to two circles and result of running the $k$-means algorithm for $k = 2$.}
\label{fig:kemans_circles}
\end{figure*}

In this case the $k$-means algorithm will fail to split the two circles as the clusters will always be convex and therefore split the data along a line.


\subsection{Linkage Clustering}
\label{section__linkage_clustering}
Another way of clustering data points is based on graph theory and the idea of consecutively connecting points \cite[Sec.~4.2.2]{Everitt2011}.

A dendrogram is in this context a (rooted) binary tree where the leaves are the data points. We will later define a more general notion of a dendrogram in section \ref{section__partitions}.



Let $x_1, \dots, x_n \in \R^d$ be a set of distinct data points and $d$ a metric on $\R^d$.
\begin{definition}{Linkage Clustering }{linkage_clustering}
Let $\mathfrak{d}$ be a \emph{distance function}\footnote{In this case, a \emph{distance function} is just a function taking values in $\R_{\geq0}$.} between clusters. We start with the partition $B^{(0)}_j := \{x_j\}$ for $j = 1, \dots, n$.
Given a partition
$$
B^{(k)}_1, \dots, B^{(k)}_{n - k} \quad \text{of} \quad x_1, \dots, x_n
$$
we successively merge the clusters $i$ and $j$ such that
$$
\mathfrak{d}(B^{(k)}_i,B^{(k)}_j)
$$
is minimal.
\end{definition}

There are multiple distance functions that we could use in the previous definition, \eg :
\begin{align*}
\mathfrak{d}_\mathrm{min} (B_i, B_j) &:= \min\{d(x,y): x \in B_i, y \in B_j\}\\
\mathfrak{d}_\mathrm{max} (B_i, B_j) &:= \max\{d(x,y): x \in B_i, y \in B_j\}\\
\mathfrak{d}_\mathrm{avg}(B_i,B_j) &:= \frac{1}{|B_i||B_j|} \sum_{x \in B_i, y \in B_j} d(x,y)\\
\end{align*}
If we use $\mathfrak{d}_\mathrm{min}$ we call it \emph{single-linkage clustering}, when using $\mathfrak{d}_\mathrm{max}$ we call it \emph{complete-linkage clustering} and \emph{average-linkage clustering} if we use $\mathfrak{d}_\mathrm{avg}$.

As an example, consider the data points in figure \ref{fig:sl_clustering} and their corresponding dendrogram obtained from single-linkage clustering.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node[circle,fill=black,inner sep=1pt, label={below:\small$v$}] (v) at (0, 0) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$w$}] (w) at (0, 1.5) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$x$}] (x) at (2.25, 0.75) {};
        \node[circle,fill=black,inner sep=1pt, label={below:\small$y$}] (y) at (6, 0) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$z$}] (z) at (6, 1.5) {};

        \draw[dashed, color=mypurple, >=stealth] (v) -- (w) node[midway, above] {\tiny{$d(v,w) = 1$}};

        \draw[dashed, color=mypurple, >=stealth] (w) -- (x) node[midway, above] {\tiny{$d(w,x) = 2$}};
        \draw[dashed, color=mypurple, >=stealth] (v) -- (x) node[midway, below] {\tiny{$d(v,x) = 2$}};

        \draw[dashed, color=mypurple, >=stealth] (x) -- (z) node[midway, above] {\tiny{$d(x,z) = 3$}};
        \draw[dashed, color=mypurple, >=stealth] (x) -- (y) node[midway, below] {\tiny{$d(x,y) = 3$}};

        \draw[<->, dashed, color=mypurple, >=stealth] (z) -- (y) node[midway, above] {\tiny{$d(y,z) = 1$}};
    \end{tikzpicture}
    \caption*{Data points $\{v,w,x,y,z\} \subset \R^2$.}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node (v) at (0,0) {v};
        \node (w) at (1,0) {w};
        \node (x) at (2,0) {x};
        \node (y) at (3,0) {y};
        \node (z) at (4,0) {z};
        
        \draw  (v) |- (0.5,1);
        \draw  (w) |- (0.5,1);

        \draw  (0.5,1) |- (1,1.5);
        \draw  (x) |- (1,1.5);
    
        \draw (y) |- (3.5,1);
        \draw (z) |- (3.5,1);

        \draw (3.5,1) |- (3,2);
        \draw (1,1.5) |- (3,2);

        \draw[dashed]  (2,2) -- (2,2.5);
    \end{tikzpicture}
    \caption*{Linkage clustering with $\mathfrak{d}_\mathrm{min}$.}
\end{subfigure}
\caption{Example of single-linkage clustering.}
\label{fig:sl_clustering}
\end{figure}

A downside of this definition of linkage clustering is that the output will always be a binary tree. In particular, if we have three equidistant points it is unclear which should be merged first. We can fix this by introducing a \emph{tie-breaking} rule or relaxing the definition of a dendrogram to allow merging more than two clusters at a time. We will see more about this once 
we define \emph{hierarchical clustering functors} in chapter \ref{chapter__clustering_functor}.

Some other problems with linkage clustering are that in the case of single-linkage clustering it has a lack of sensitivity for density, and it has a tendency to produce long chains as clusters. Complete-linkage and average-linkage clustering have nice properties in regard to density, but it was show that they are not stable under small perturbations of the data \cite[Sec.~3.6]{JMLR:v11:carlsson10a}, \cite{Lance1967-ci}.

\section{Invariance of Clustering Algorithms}
\label{seciton__preserving_structure}
An important aspect of clustering methods is that they remain invariant under certain transformations.
The idea is that if a clustering algorithm is invariant under certain transformations (which we can think of as noise) it captures information of the remaining features (which hopefully are not noise).

\begin{example}{}{}
Consider a finite set $S \subset \R^2$ representing people at a gathering. When searching for a clustering algorithm to detect friendship groups, it would make sense to only consider algorithms with the following invariances:
\begin{itemize}
    \item Translation Invariance
    \item Rotation Invariance
    \item Scale Invariance
\end{itemize}
\Ie\ for any $v \in \R^2$, $R \in \mathrm{SO}_2(\R)$ and $\lambda > 0$ we would like our clustering algorithm to produce the same partition on the data $v + S$, $R \circ S$ and $\lambda \cdot S$. Here the intuition should be that such attributes are merely artifacts of the data collection process, in particular the choice of coordinate system.
\end{example}

\subsection{Kleinberg's Impossibility Theorem}
It is apparent that as we require more invariants from a clustering algorithm it becomes harder to construct one.
There exists an important result in this regard.
\textsc{Kleinberg} showed that for certain seemingly reasonable properties there exist no clustering algorithms satisfying them \cite{Kleinberg2002}.
In the context of this theorem we define a clustering algorithm to be a procedure that assigns to a finite metric space $(X,d)$ a partition of $X$\footnote{In chapter \ref{chapter__clustering_functor} we will formalize the definition of a clustering algorithm.}.

Here are the properties \textsc{Kleinberg} considered:

\begin{definition}{Scale Invariance}{}
We say that a clustering algorithm $\Cf$ is \emph{scale invariant} if for every finite metric space $(X,d)$ and $\lambda > 0$ we have
$$
\Cf(X,d) = \Cf(X, \lambda \cdot d).
$$
\Ie\ re-scaling the metric does not affect the clustering algorithm.
\end{definition}

\begin{definition}{Richness}{richness}
A clustering algorithm $\Cf$ is \emph{rich} if for every set $X$ and every partition $P$ of $X$ there exists a metric $d$ on $X$ such that $\Cf(X,d) = P$.
\end{definition}

\begin{definition}{Consistency}{}
Let $\Cf$ be a clustering algorithm. We say that it is \emph{consistent} if for every finite metric space $(X,d)$ and every metric $d'$ on $X$ such that
\begin{itemize}
    \item $d'(x,y) \leq d(x,y)$ if $x,y$ are in the same part of $\Cf(X,d)$
    \item $d'(x,y) \geq d(x,y)$ if $x,y$ are in different parts of $\Cf(X,d)$
\end{itemize}
we have $\Cf(X,d') = \Cf(X,d)$.
\end{definition}

Kleinberg showed that in combination these properties are impossible to satisfy.

\begin{theorem}{Kleinberg \cite[Thm.~2.1]{Kleinberg2002}}{kleinberg}
There exists no clustering algorithm that is scale invariant, rich and consistent at the same time.
\end{theorem}

Furthermore, if we drop any of the requirements we can find clustering algorithms satisfying the remaining two properties. As an example the $k$-means algorithm \ref{def:kmeans_clustering} is clearly not rich since we restrict ourselves to partitions with $k$ parts. It is however scale invariant.