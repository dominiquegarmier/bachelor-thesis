\chapter{Data Clustering}
\label{chapter__dataclustering}

\section{Overview of Clustering Algorithms}
We would like to present a few clustering methods to contrast against the methods we will present in the latter half of this thesis. These are commonly studied in machine learning and data science.

\subsection{$k$-means}
A common method of clustering is to find a $k$-partition of the points according to some criterion.
One such example is the \emph{$k$-means} clustering objective \cite[Sec.~3.1]{Scitovski2021}.

Let $x_1, \dots, x_n \in \R^d$ be a set of points.

\begin{definition}{$k$-means Clustering}{kmeans_clustering}
For $k \in \N$ the \emph{$k$-means clustering objective} refers to finding a partition $C_1, \dots, C_k$ of $\{x_1, \dots, x_n\}$ such that
\begin{equation*}
\label{eq:kmean_optimization}
    \sum_{i = 1}^k \sum_{x \in C_i} \|x - \mu_i\|^2
\end{equation*}
is minimal, where $\mu_k := \frac{1}{|C_k|}\sum_{x \in C_k} x$ is the \emph{center} of $C_k$\footnote{In case $C_k = \emptyset$ we set $\mu_k = 0 \in \R^d$.}.
\end{definition}
To find such a partition we would have to check all possible partitions of $x_1, \dots, x_n$. This is computationally unfeasible. In practice, we use approximate methods such as \textsc{Lloyd}'s algorithm \cite[Sec.~3.1.2]{Scitovski2021}.
Sometimes it is simply referred to as \emph{the $k$-means algorithm}.

\subsection{Linkage-Clustering}
\label{section__linkage_clustering}
Another way of clustering data points is based on graph theory and the idea of consecutively connecting points \cite[Sec.~4.2.2]{Everitt2011}.
As an output of this method we obtain a
dendogram\footnote{See section \ref{section__partitions} for an in-depth definition of dendograms.}.

Let $x_1, \dots, x_n \in \R^d$ be a set of distinct data points and $d$ a metric on $\R^d$.
\begin{definition}{Linkage-Clustering }{linkage_clustering}
Let $\mathfrak{d}$ be a \emph{distance function}\footnote{In this case, a \emph{distance function} is just a function taking values in $\R_{\geq0}$.} between clusters. We start with the partition $B^{(0)}_j := \{x_j\}$ for $j = 1, \dots, n$.
Given a partition
$$
B^{(k)}_1, \dots, B^{(k)}_{n - k} \quad \text{of} \quad x_1, \dots, x_n
$$
we successively merge the clusters $i$ and $j$ such that
$$
\mathfrak{d}(B^{(k)}_i,B^{(k)}_j)
$$
is minimal.
\end{definition}

There are multiple distance functions that we could use in the previous definition, \eg :
\begin{align*}
\mathfrak{d}_\mathrm{min} (B_i, B_j) &:= \min\{d(x,y): x \in B_i, y \in B_j\}\\
\mathfrak{d}_\mathrm{max} (B_i, B_j) &:= \max\{d(x,y): x \in B_i, y \in B_j\}\\
\mathfrak{d}_\mathrm{avg}(B_i,B_j) &:= \frac{1}{|B_i||B_j|} \sum_{x \in B_i, y \in B_j} d(x,y)\\
\end{align*}
If we use $\mathfrak{d}_\mathrm{min}$ we call it \emph{single-linkage clustering}, when using $\mathfrak{d}_\mathrm{max}$ we call it \emph{complete-linkage clustering} and \emph{average-linkage clustering} if we use $\mathfrak{d}_\mathrm{avg}$.

\section{Preserving Structure}
\label{seciton__preserving_structure}
An important aspect of clustering methods is that they remain invariant under certain transformations.
The idea is that if a clustering algorithm is invariant under certain transformations (which we can think of as noise) it captures information of the remaining features (which hopefully are not noise).

\begin{example}{}{}
Consider a finite set $S \subset \R^2$ representing people at a gathering. When searching for a clustering algorithm to detect friendship groups, it would make sense to only consider algorithms with the following invariances:
\begin{itemize}
    \item Translation Invariance
    \item Rotation Invariance
    \item Scale Invariance
\end{itemize}
The intuition being that such attributes are merely artifacts of the data collection process.
\end{example}

\subsection{Kleinberg's Impossibility Theorem}
It is apparent that as we require more invariants from a clustering algorithm it becomes harder to construct one.
There exists an important result in this regard.
\textsc{Kleinberg} showed that for certain seemingly reasonable properties there exist no clustering algorithms satisfying them \cite{Kleinberg2002}.
In the context of this theorem we define a clustering algorithm to be a procedure that assigns to a finite metric space $(X,d)$ a partition of $X$\footnote{In chapter \ref{chapter__clustering_functor} we will formalize the definition of a clustering algorithm.}.

Here are the properties \textsc{Kleinberg} considered:

\begin{definition}{Scale Invariance}{}
We say that a clustering algorithm $\Cf$ is \emph{scale invariant} if for every finite metric space $(X,d)$ and $\lambda > 0$ we have
$$
\Cf(X,d) = \Cf(X, \lambda \cdot d).
$$
\Ie\ re-scaling the metric does not affect the clustering algorithm.
\end{definition}

\begin{definition}{Richness}{richness}
A clustering algorithm $\Cf$ is \emph{rich} if for every set $X$ and every partition $P$ of $X$ there exists a metric $d$ on $X$ such that $\Cf(X,d) = P$.
\end{definition}

\begin{definition}{Consistency}{}
Let $\Cf$ be a clustering algorithm. We say that it is \emph{consistent} if for every finite metric space $(X,d)$ and every metric $d'$ on $X$ such that
\begin{itemize}
    \item $d'(x,y) \leq d(x,y)$ if $x,y$ are in the same part of $\Cf(X,d)$
    \item $d'(x,y) \geq d(x,y)$ if $x,y$ are in different parts of $\Cf(X,d)$
\end{itemize}
we have $\Cf(X,d') = \Cf(X,d)$.
\end{definition}

Kleinberg showed that in combination these properties are impossible to satisfy.

\begin{theorem}{Kleinberg \cite[Thm.~2.1]{Kleinberg2002}}{kleinberg}
There exists no clustering algorithm that is scale invariant, rich and consistent at the same time.
\end{theorem}

Furthermore, if we drop any of the requirements we can find clustering algorithms satisfying the remaining two properties. As an example the $k$-means algorithm \ref{def:kmeans_clustering} is clearly not rich since we restrict ourselves to partitions with $k$ parts. It is however scale invariant.

\source[is kmeans consistent? dont want to prove this, probably very hard]