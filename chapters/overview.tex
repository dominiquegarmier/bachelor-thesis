\chapter{Data Clustering}
\label{chapter__dataclustering}

Data clustering refers to the procedure of assigning to a set of data points some kind of ``grouping''. 
This is often used as a first step in machine learning pipelines as a way to reduce data complexity.
In the simplest case this ``grouping'' is a \emph{partition} of the original data points. This is also called \emph{classical clustering} (see Figure \ref{fig:intro_partition}).
Alternatively, we could assign a so-called \emph{dendrogram}
to the data points, which is referred to as \emph{hierarchical clustering} (see Figure \ref{fig:intro_dendrogram}). The first part this chapter explains these concepts in more detail.

Next, we discuss two examples of commonly studied clustering algorithms. Namely, \emph{$k$-means clustering}, a classical clustering technique and \emph{linkage clustering}, a hierarchical clustering algorithm.
%
By contextualizing these algorithms with computations we demonstrate how they can fall short in certain cases.
This part is based on the books \cite{Everitt2011} and \cite{Scitovski2021}.

Finally, we show how clustering algorithms can be seen as remaining invariant under certain transformations of their input.
%
One such transformation could be the introduction of noise into the data.
If we model this noise as some transformation of the ``ground truth'' we can then try to construct a clustering algorithm that is invariant under this transformation. In other words, the output of our clustering algorithm would not depend on the noise. This idea naturally leads to \textsc{Kleinberg}'s impossibility result \cite{Kleinberg2002}.

\Needspace{16\baselineskip}

\section{Partitions, Dendrograms and Clustering Algorithms}
\label{section__partitions}
In this section, we make precise what we mean by a ``partition'' or a ``dendrogram'', which will enable us to formally define \emph{clustering algorithms}.

\begin{definition}{Partitions of Finite Sets}{part_of_finite_sets}
Let $X$ be a finite set. A \emph{partition} $P$ of $X$ is the set of equivalence classes $X/_{\sim_P}$ for some equivalence relation $\sim_P$ on $X$. The set of all such partitions is denoted by $\P(X)$.\par

\medskip Furthermore:
\begin{itemize}
    \item We interchangeably use $P$ (the partition) and $\sim_P$ (the corresponding equivalence relation).
    \item A single equivalence class $X_\alpha \in P$ will be called a \emph{block} or \emph{part} of $P$.
\end{itemize}
\end{definition}

It naturally makes sense to use partitions as outputs of (classical) clustering algorithms, where a block of a partition is to be interpreted as a \emph{cluster}.

When describing a partition it is often easier to describe a minimal set of conditions the partition relation must satisfy. More formally, we talk about taking the \emph{transitive closure} of a relation.

\begin{definition}{}{trans_closure}
    Given a relation $\sim$ on a set $X$ its \emph{transitive closure} is the transitive relation $\sim^+$ such that for $x,y \in X$ we have
    $$
    x \sim^+ y
    $$
    if there exists a sequence $x = x_0, x_1, \dots, x_n = y$ with $x_i \sim_R x_{i+1}$ for all $i = 0, \dots, n-1$ \cite[p.337]{Lidl1997-kc}. In this case we also say that $\sim^+$ is \emph{generated} by $\sim$.
\end{definition}

In particular, if $\sim$ is reflexive and symmetric then $\sim^+$ is an equivalence relation.
Later we will use this fact to define equivalence relations as transitive closures of reflexive and symmetric relations.

\begin{example}{}{}
Let $G = (V,E)$ be a graph, then $E$ is a binary symmetric relation on $V$. Two points $v,w \in V$ lie in the same connected component of $G$ if there exists a path, \ie, a sequence $v =v_0, v_1, \dots, v_n = w$ such that $(v_i, v_{i+1}) \in E$ for all $i = 0, \dots, n-1$. The resulting partition of $V$ into connected components corresponds to the transitive closure of $E$.
\end{example}

To define a \emph{dendrogram} we need to give a precise meaning to what we previously informally described as the ``merging'' of clusters.

\begin{definition}{Refinement}{refinement}
Let $P, Q \in \mathfrak{P}(X)$. Then we write $P \refines Q$ and say that $P$ \emph{refines} $Q$ if
\begin{equation*}
    \forall x,y \in X: x \sim_P y \implies x \sim_Q y.
\end{equation*}
This defines a partial order on $\P(X)$.
\end{definition}

\begin{example}{}{}
Consider $X := \{a,b,c\}$ together with partitions
$$
P := \{\{a\}, \{b\}, \{c\}\} \quad \text{and} \quad Q := \{\{a,b\}, \{c\}\}.
$$
As the blocks in $P$ are always contained in some block of $Q$ we have $P \refines Q$.
\end{example}
This corresponds to the idea of ``merging'' in the sense that if we have $P,Q \in \P(X)$ such that $P \refines Q$ we can say that $Q$ can be optained from $P$ by ``merging'' blocks of $P$.

There are two extreme kinds of partitions: one where all points belong to the same equivalence class, and another where each point forms its own equivalence class. For this we introduce the following notation.

\begin{definition}{}{}
We say that $P \in \P(X)$ is:
\begin{enumerate}
    \item \emph{discrete} if $x \sim_P y \iff x = y$.
    \item \emph{trivial} if $x \sim_P y$ for all $x,y \in X$.
\end{enumerate}
\end{definition}

Naturally, if $Q$ is discrete and $P$ is trivial then $Q \refines R \refines P$ for all $R \in \P(X)$.

With this we now have everything to define a \emph{dendrogram}.

\begin{definition}{Dendrogram \cite[Def.~2.2]{Carlsson2010}}{dendrogram}
Let $X$ be a finite set. A map $\theta: \R_{\geq0} \to \mathfrak{P}(X)$ with
\begin{enumerate}
    \item $\forall r,s \in \R_{\geq0}: r \leq s \implies  \theta(r) \refines \theta(s)$,
    \item $\exists r,s \in \R_{\geq0}$ such that $\theta(r)$ is trivial and $\theta(s)$ is discrete\footnote{In particular, we get that $\theta(0)$ is always discrete.},
    \item $\forall r \in \R_{\geq0} \,\exists \varepsilon > 0$ such that $\theta$ is constant on $[r, r + \varepsilon)$,
\end{enumerate}
is called a \emph{dendrogram} of $X$. Sometimes $r$ is referred to as the \emph{scale}.
\end{definition}

This will be familiar to anyone who has seen \emph{persistent homology} where such properties are referred to as \emph{persistence} \cite[Chap.~3]{Carlsson2014}.

As for the relevance of the last condition, consider the following remark.

\begin{myremark}{}{regularity_of_dendrograms}
Since $X$ is a finite set, we could decide to omit the third condition in the definition of a dendrogram.
Dendrograms would still have discrete scale \ie\ the dendrogram would still be constant on intervals.
However, it would not be clear what value the dendrogram would take at the endpoints of these intervals.
This technicality will become important for some uniqueness theorems we present later.
\end{myremark}

\begin{example}{}{}
As an example of a dendrogram consider Figure \ref{fig:dendrogram_example}.
\begin{center}
\begin{minipage}{\linewidth}
\centering
\begin{tikzpicture}
    \node (a) at (0,-0.25) {a};
    \node (b) at (1,-0.25) {b};
    \node (c) at (2,-0.25) {c};
    \node (d) at (3,-0.25) {d};
    
    \draw  (a) |- (0.5,0.5);
    \draw  (b) |- (0.5,0.5);

    \draw  (0.5,0.5) |- (1,1.25);
    \draw  (c) |- (1,1.25);


    \draw  (1,1.25) |- (1.5,2);
    \draw (d) |- (1.5,2);

    \draw[dashed]  (1.5,2) -- (1.5,2.5);
    \draw[->, >=stealth] (-0.5,-0.25) -- (-0.5,2.5);

    \draw[dashed] (-0.25, 0.6) -- (3.25, 0.6);
    \draw[color=myred, opacity=0.5, line width=1mm] (-0.2, 0.6) -- (1.2, 0.6);
    \draw[color=myblue, opacity=0.5, line width=1mm] (1.8, 0.6) -- (2.2, 0.6);
    \draw[color=mygreen, opacity=0.5, line width=1mm] (2.8, 0.6) -- (3.2, 0.6);

    \draw[dashed] (-0.25, 1.9) -- (3.25, 1.9);
    \draw[color=mypurple, opacity=0.5, line width=1mm] (-0.2, 1.9) -- (2.2, 1.9);
    \draw[color=mygreen, opacity=0.5, line width=1mm] (2.8, 1.9) -- (3.2, 1.9);

    \draw[] (-0.55, 0.6) -- node[left]{$1$} (-0.45, 0.6);
    \draw[] (-0.55, 1.9) -- node[left]{$2$} (-0.45, 1.9);

    \draw[|->, >=to] (3.35, 0.6) -- (4.25, 0.6);
    \draw[|->, >=to] (3.35, 1.9) -- node[above]{$\theta$} (4.25, 1.9);

    \node[circle,fill=black,inner sep=1pt] (z) at (5, 0.6) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (6, 0.6) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (7, 0.6) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (8, 0.6) {};

    \draw[ellipse, dashed, fill=myred, fill opacity=0.15] (5.5, 0.6) ellipse (0.75 and 0.25);
    \draw[ellipse, dashed, fill=myblue, fill opacity=0.15] (7, 0.6) ellipse (0.25 and 0.25);
    \draw[ellipse, dashed, fill=mygreen, fill opacity=0.15] (8, 0.6) ellipse (0.25 and 0.25);

    \node[circle,fill=black,inner sep=1pt] (z) at (5, 1.9) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (6, 1.9) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (7, 1.9) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (8, 1.9) {};

    \draw[ellipse, dashed, fill=mypurple, fill opacity=0.15] (6, 1.9) ellipse (1.25 and 0.25);
    \draw[ellipse, dashed, fill=mygreen, fill opacity=0.15] (8, 1.9) ellipse (0.25 and 0.25);

    \node (a) at (5,-0.25) {a};
    \node (b) at (6,-0.25) {b};
    \node (c) at (7,-0.25) {c};
    \node (d) at (8,-0.25) {d};

        \node[rotate=90] at (6.5, 1.25) {$\preceq$};
\end{tikzpicture}
\captionof{figure}{A dendrogram $\theta: \R_{\geq0} \to \P(X)$ with the four points $X = \{a,b,c,d\}$.}
\label{fig:dendrogram_example}
\end{minipage}
\end{center}
Notice how in this case the monotonicity condition is satisfied \eg\ $\theta(1) \refines \theta(2)$ as shown on the righthand side. The second condition of a dendrogram is also met as it becomes trivial at the top and discrete at the bottom. Finally, the third condition tells us what value the dendrogram takes whenever a ``merge happens''.
\end{example}

With this we can now state the definition of a \emph{clustering algorithm}.

\begin{definition}{Clustering Algorithm}{}
A \emph{clustering algorithm} $\Cf$ is a procedure that assigns to a finite metric space $(X,d)$ either a partition or dendrogram, which we denote by
$$
\Cf(X,d).
$$
In the case that $\Cf$ outputs partitions, we refer to it as a \emph{classical} clustering algorithm. If it outputs dendrograms, we refer to it as \emph{hierarchical}.
\end{definition}

In Chapter \ref{chapter__clustering_functor} we will revisit this definition, at which point we will call it a \emph{clustering functor}.

\Needspace*{7\baselineskip}

\section{Examples of Clustering Algorithms}

Let us consider a few examples of clustering algorithms.

\subsection[$k$-means]{$\boldsymbol{k}$-means}
A common method of clustering is to find a partition of the data into $k$ parts that minimizes some objective function.
One such example is the \emph{$k$-means} clustering objective \cite[Sec.~3.1]{Scitovski2021}.

\begin{definition}{$k$-means Clustering}{kmeans_clustering}
Let $\{x_1, \dots, x_n\} \subset \R^d$. For $k \in \N$ \emph{$k$-means clustering} refers to finding a partition $C_1, \dots, C_k$ of $\{x_1, \dots, x_n\}$ such that
\begin{equation*}
\label{eq:kmean_optimization}
    \sum_{i = 1}^k \sum_{x \in C_i} \|x - \mu_i\|^2_2
\end{equation*}
is minimal, where $\mu_k := \frac{1}{|C_k|}\sum_{x \in C_k} x$ is the \emph{center} of $C_k$\footnote{In case $C_k = \emptyset$ we set $\mu_k = 0 \in \R^d$.}.
\end{definition}
To find such a partition we would have to check all possible partitions of $x_1, \dots, x_n$. This is computationally infeasible. In practice, we use approximate methods such as \textsc{Lloyd}'s algorithm, which is sometimes simply referred to as \emph{the $k$-means algorithm} \cite[Sec.~3.1.2]{Scitovski2021}.

\begin{definition}{Lloyd's Algorithm}{lloyds_algorithm}
Given $\{x_1, \dots, x_n\} \subset \R^d$ and some \emph{centers} $z_1, \dots, z_k \in \R^d$
Lloyd's algorithm consists of iteratively applying the following two steps:

\begin{enumerate}
    \item \textbf{Assignment Step:} Assign each point $x_i$ to the cluster $C_j$ such that
    $$
    j = \argmin_{1 \leq l \leq k} \|x_i - z_l\|^2_2.
    $$

    \item \textbf{Update Step:} Update the centers $z_1, \dots, z_k$ by setting
    $$
    z_j = \frac{1}{|C_j|}\sum_{x \in C_j} x \quad \forall j \in \{1, \dots, k\},
    $$
    where $z_j = 0$ if $C_j = \emptyset$. 
\end{enumerate}
\end{definition}

One can show that Lloyd's algorithm converges after finitely many steps \cite[Thm.~3.14]{Scitovski2021}.
We will see that the choice of starting centers $z_1, \dots, z_k$ can have a significant impact on the outcome of the algorithm.

\begin{example}{}{}
Consider the set $\{(0,0), (0,1), (2,0), (2,1)\} \subset \R^2$, $k = 2$ and two possible choices of starting centers:

\begin{enumerate}
    \item $z_1 = (0, 1/2)$ and $z_2 = (2, 1/2);$
    \item $z_1' = (1, 0)$ and $z_2' = (1, 1).$
\end{enumerate}

Consider the first case. In the first assignment step we assign $(0,0)$ and $(0,1)$ to the first cluster $C_1$ and similarly $(2,0)$ and $(2,1)$ to the second cluster $C_2$. In the update step we update the centers and get $z_1 = \frac{1}{2}\left[(0,0) + (0,1)\right] = (0,1/2)$. Similarly, we get $z_2 = (2,1/2)$. Notice that these are the same centers as we started with. Therefore, the algorithm converges after just one iteration.
\medskip

Repeating the same computations for the second case we notice that the algorithm will also converge after just one step to the original centers. This will result in the red and blue clusters as seen in Figure \ref{fig:kmeans_example}.

%\begin{enumerate}
%    \item $C_1 = \{(0,0), (0,1)\}$ and $C_2 = \{(2,0), (2,1)\}$
%    \item $C_1' = \{(0,0), (2,0)\}$ and $C_2' = \{(0,1), (2,1)\}$
%\end{enumerate}

\begin{center}
\begin{minipage}{\linewidth}
\centering
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
        \node[circle,fill=black,inner sep=1pt] at (0,0) {};
        \node[circle,fill=black,inner sep=1pt] at (0,1.5) {};
        \node[circle,fill=black,inner sep=1pt] at (3,0) {};
        \node[circle,fill=black,inner sep=1pt] at (3,1.5) {};
    
        \node[circle,fill=myblue,inner sep=1pt, label={[myblue]:\small$z_1$}] at (0,0.75) {};
        \node[circle,fill=myred,inner sep=1pt, label={[myred]:\small$z_2$}] at (3,0.75) {};
    
        \draw[ellipse,dashed,fill=myblue,fill opacity=0.15] (0,0.75) ellipse (0.75 and 1.5);
        \draw[ellipse,dashed,fill=myred,fill opacity=0.15] (3,0.75) ellipse (0.75 and 1.5);
    \end{tikzpicture}\\
    \begin{center}
         starting centers $(1)$
    \end{center}
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
        \node[circle,fill=black,inner sep=1pt] at (0,0) {};
        \node[circle,fill=black,inner sep=1pt] at (0,1.5) {};
        \node[circle,fill=black,inner sep=1pt] at (3,0) {};
        \node[circle,fill=black,inner sep=1pt] at (3,1.5) {};
    
        \node[circle,fill=myblue,inner sep=1pt, label={[myblue]:\small$z_1'$}] at (1.5,0) {};
        \node[circle,fill=myred,inner sep=1pt, label={[myred]:\small$z_2'$}] at (1.5,1.5) {};
    
        \draw[ellipse,dashed,fill=myblue,fill opacity=0.15] (1.5,0) ellipse (2 and 0.65);
        \draw[ellipse,dashed,fill=myred,fill opacity=0.15] (1.5,1.5) ellipse (2 and 0.65);
    \end{tikzpicture}\\
    \begin{center}
         starting centers $(2)$
    \end{center}
\end{minipage}
\captionof{figure}{$k$-means clustering for different starting centers.}
\label{fig:kmeans_example}
\end{minipage}
\end{center}

In fact the first solution is the one minimizing the $k$-means objective. So we see that the choice of initial centers is important for the outcome of the algorithm. For this reason in practice one might run the algorithm multiple times with different initial centers drawn from some distribution and then choose the best solution.
\end{example}

Another potential downside of the $k$-means algorithm is that its clusters are always convex.
\begin{example}{}{}
Consider the following data points in Figure \ref{fig:kemans_circles} for $k = 2$.

\begin{center}
\begin{minipage}{\linewidth}
\centering
\begin{tikzpicture}
    \draw[circle, dashed](0,0) circle (1);
    \draw[circle, dashed](0,0) circle (2);

    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0095579832468025,-0.23458616178292213) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8621356010904919,-0.43521673725850546) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8627616584531776,0.4128659891205529) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8291421634685034,0.5054463113114217) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9764896823801532,0.12472092624358487) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0140626107241928,-0.13705972885468481) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.7797471398667898,0.7164817919382073) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5877999996419829,0.7418100674775248) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9897424809214725,-0.35865675131466895) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.0922792389809537,1.006191078288321) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9935603794343448,0.11784757153395319) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.026524547115352,-0.37871897925085024) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9488139684656478,0.7747854617416964) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5164088657878309,2.03153994637684) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.429935616806283,1.5022872229353683) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.8365483721220037,1.9189627548136554) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.981301736481553,-0.14499075605710074) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6349515928709943,1.2202599759050379) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5231409868905119,1.9793981704336547) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.053624362266639,1.6908381811818811) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.04231412307509439,2.045672752950578) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.06811082126040592,1.9438370464512291) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0906354601250503,1.694220851300212) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.589017397924315,1.3208705982413913) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.2875236513311277,1.5206616702727054) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.6515674232388111,1.8678604281807427) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6558729051313483,1.1575803869149104) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6685583816277019,1.1508647790179702) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9448141306887845,-0.28977280114742615) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9472636540198844,-0.28636654246930904) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9295785207047487,0.48494891715335603) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.41340373800077374,1.97704090631188) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.416451128927296,1.3334319841181093) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.5504121965152975,1.8450892982198965) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.484969911606966,1.3723065854767136) {};
    % -----------------
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.914573789718945,-0.28964318073223894) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.1982533125570065,-0.8919178733968665) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.80577538812941,-0.607080998055582) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9412027083485228,-0.27269568749492573) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8148270419175926,0.4238878668815002) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6225375984421799,-0.8119486329925646) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8821678647126937,0.3648396878561235) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.7795857969200991,-0.6304341921488942) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7483950763114702,0.6729373613544269) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.2739384002725763,-1.0208542531185714) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9228380796524287,-0.16940862942773213) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9282962576231144,-0.42297153472927584) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.49750583441724194,-0.8706964828847926) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.16573977530367748,-0.9270432989794191) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.7154381602074034,-0.761747732857447) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.28063484315157317,-0.8794997600003289) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.0430965817766624,0.2575111200438646) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.20871850422394092,-1.0747146100323817) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6320684095077692,-0.7632679571994616) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9706230943148069,-0.13408763258353848) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9122031196423117,0.5420940005463377) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.15908376563698895,-1.0366970717997068) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9794313663308976,0.2768904673101241) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9410855486192096,-0.43158246153463864) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8050509365055522,-0.7337938215702184) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7668888938180021,-0.6516232811273315) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7569340532084752,-0.683852150730453) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.3874097599372178,-0.8483389110540501) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.2297367654792433,1.4958896410698033) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.8902258184052567,0.7894776893682616) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6273208428361784,-1.8371947162534354) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9885817783568538,-1.7147623663359197) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9386023582122706,-1.7236389913807584) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.3401106297221774,1.5451099391524472) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.7696595846944951,-0.835412527730738) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.94149620924287,0.7884962462992549) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.1613066182943586,-1.6658708071329087) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.9737616591333986,0.42415122258148036) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.0910291132157077,-1.6346094222929535) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.420125841706075,1.5367643867093461) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.8773373323041873,-1.83463023722728) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (1.0295543643648137,-1.6695857338970421) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.9628887251833387,0.3917097103265355) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.6082435557290202,-1.2449832245663626) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.1367588700508612,-2.0242149878928153) {};

    \draw[dashed, color=myred, line width=1] (-1.9, 2.4) -- (2.2, -2); % diff = (4.1, -4.4)
    \draw[->, color=myred, >=stealth, line width=1] (-1.9 + 0.41, 2.4 - 0.44) -- (-1.9 + 0.41 + 0.22, 2.4 - 0.44 + 0.205);
    \draw[->, color=myred, >=stealth, line width=1] (2.2 - 0.41, -2 + 0.44) -- (2.2 - 0.41 + 0.22, -2 + 0.44 + 0.205);
\end{tikzpicture}
\captionof{figure}{Points generated by adding noise to two circles and the result of running the $k$-means algorithm for $k = 2$.}
\label{fig:kemans_circles}
\end{minipage}
\end{center}
In this case, the  $k$-means algorithm will be unable to separate the two circles because it produces convex clusters, causing it to divide the data along a straight line.
\end{example}

\subsection{Linkage Clustering}
\label{section__linkage_clustering}
Let us now consider a clustering algorithm that produces a dendrogram as an output. We achieve this by consecutively merging clusters based on some distance criterion \cite[Sec.~4.2.2]{Everitt2011}.

\begin{definition}{Linkage Clustering }{linkage_clustering}
Let $X := \{x_1, \dots, x_n\} \subset \R^n$ and $\mathfrak{d}\colon\mathscr{P}(X) \times \mathscr{P}(X) \to \R_{\geq0}$\footnote{$\mathscr{P}(X)$ denotes the power set of $X$.} a \emph{distance function} between clusters. We start with the partition
$$
B^{(0)}_j := \{x_j\}$$
for $j \in \{1, \dots, n\}$. Given a partition
$$
B^{(k)}_1, \dots, B^{(k)}_{n - k} \quad \text{of} \quad x_1, \dots, x_n
$$
we successively merge the clusters $j$ and $\ell$ such that
$$
\mathfrak{d}(B^{(k)}_j,B^{(k)}_\ell)
$$
is minimal for $j,\ell \in \{1, \dots, n - k\}$.
\end{definition}
The reason this is infact a hierarchical clustering algorithm, is that we can now construct the dendrogram $\theta: \R_{\geq0} \to \P(X)$ given by
$$
\theta(t) := \{B_1^{(k)}, \dots, B_{n-k}^{(k)}\} \in \P(X)
$$
for all $t \in [k, k+1)$ and $k = 0, \dots, n-1$.


Depending on the distance function $\mathfrak{d}$ we use we call \emph{linkage clustering}

\begin{itemize}
    \item \emph{single-linkage clustering} if $\mathfrak{d}_\mathrm{min} (B_i, B_j) := \min\{d(x,y): x \in B_i, y \in B_j\}$;
    \item \emph{complete-linkage clustering} if $\mathfrak{d}_\mathrm{max} (B_i, B_j) := \max\{d(x,y): x \in B_i, y \in B_j\}$;
    \item \emph{average-linkage clustering} if $\mathfrak{d}_\mathrm{avg}(B_i,B_j) := \frac{1}{|B_i||B_j|} \sum_{x \in B_i, y \in B_j} d(x,y)$.
\end{itemize}

\begin{example}{}{}
As an example, consider the data points in Figure \ref{fig:sl_clustering} and their corresponding dendrogram obtained from single-linkage clustering. Here we first merge the points $v,w$ and $z,y$ as they are the closest. Next we add the point $x$ to the cluster $\{v,w\}$ as $x$ is closer to $\{v,w\}$ than $\{y,z\}$. Finally, we merge the remaining two clusters $\{v,w,x\}$ and $\{y,z\}$ to obtain a dendrogram. Notice how this order is fully described on the righthand side.
\begin{center}
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node[circle,fill=black,inner sep=1pt, label={below:\small$v$}] (v) at (0, 0) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$w$}] (w) at (0, 1.5) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$x$}] (x) at (2.25, 0.75) {};
        \node[circle,fill=black,inner sep=1pt, label={below:\small$y$}] (y) at (6, 0) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$z$}] (z) at (6, 1.5) {};

        \draw[dashed, color=mypurple, >=stealth] (v) -- (w) node[midway, above] {\tiny{$d(v,w) = 1$}};

        \draw[dashed, color=mypurple, >=stealth] (w) -- (x) node[midway, above] {\tiny{$d(w,x) = 2$}};
        \draw[dashed, color=mypurple, >=stealth] (v) -- (x) node[midway, below] {\tiny{$d(v,x) = 2$}};

        \draw[dashed, color=mypurple, >=stealth] (x) -- (z) node[midway, above] {\tiny{$d(x,z) = 3$}};
        \draw[dashed, color=mypurple, >=stealth] (x) -- (y) node[midway, below] {\tiny{$d(x,y) = 3$}};

        \draw[<->, dashed, color=mypurple, >=stealth] (z) -- (y) node[midway, above] {\tiny{$d(y,z) = 1$}};
    \end{tikzpicture}
\end{minipage}
\hfill %
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node (v) at (0,0) {v};
        \node (w) at (1,0) {w};
        \node (x) at (2,0) {x};
        \node (y) at (3,0) {y};
        \node (z) at (4,0) {z};
        
        \draw  (v) |- (0.5,1);
        \draw  (w) |- (0.5,1);

        \draw  (0.5,1) |- (1,1.5);
        \draw  (x) |- (1,1.5);
    
        \draw (y) |- (3.5,1);
        \draw (z) |- (3.5,1);

        \draw (3.5,1) |- (3,2);
        \draw (1,1.5) |- (3,2);

        \draw[dashed]  (2,2) -- (2,2.5);
    \end{tikzpicture}
\end{minipage}
\captionof{figure}{Example of single-linkage clustering with the data points $\{v,w,x,y,z\} \subset \R^2$ on the left and the resulting dendrogram on the right.}
\label{fig:sl_clustering}
\end{center}
\end{example}

A downside of this definition of linkage clustering is that the output will always be a binary tree. In particular, if we have three equidistant points it is unclear which should be merged first. We can fix this by introducing a ``tie-breaking'' rule or by merging more than two clusters at once. We will see more about this once we define \emph{hierarchical clustering functors} in Chapter \ref{chapter__clustering_functor}.

Other problems of single-linkage clustering include its insensitivity to density and the tendency to produce long chains as clusters.
Complete-linkage and average-linkage clustering have nice properties with regard to density, but it was show that they are not stable under small perturbations of the data \cite[Sec.~3.6]{JMLR:v11:carlsson10a}, \cite{Lance1967-ci}.

\section{Invariance of Clustering Algorithms}
\label{seciton__preserving_structure}

One desirable property of a clustering algorithm is that it remains invariant under certain transformations of the data.
The reason this is desirable is that we can model noise in the data as some transformation of the underlying ``ground truth''.
If an algorithm is invariant under such transformations we can hope to recover the ground truth. As we are going to see, it is often not realistic to expect that such an algorithm even exists. But this still motivates the description of clustering algorithms in terms of their invariances.

\begin{example}{}{real_world_kleinberg}
Consider a finite set $X$ representing guests at a gathering and a metric $d$ measuring the distance between individual guests.
When searching for a (classical) clustering algorithm that finds friendship groups it would make sense to consider algorithms that have the following properties:

\begin{enumerate}
    \item If we rescale the metric $d$, \ie, we change the unit with which we measure distance, the algorithm will not change its belief.
    \item No matter what the firendship groups are, the guests could arrange themselves appropriately such that our algorithm can detect them (\ie, guests in the same group move close together).
    \item When guests that the algorithm believes belong to the same group move closer together, the algorithm will continue to classify them as part of the same group.
\end{enumerate}
\end{example}

\subsection{Kleinberg's Impossibility Theorem}
It is apparent that as we require more invariants from a clustering algorithm it becomes harder to construct one.
There exists an important result in this regard.
\textsc{Kleinberg} showed that a generalized version of the properties we considered in Example \ref{exa:real_world_kleinberg} are impossible to satisfy simultaneously \cite{Kleinberg2002}.
To state \textsc{Kleinberg}'s theorem we first need to precisely define these properties.

\begin{definition}{Scale Invariance}{}
We say that a clustering algorithm $\Cf$ is \emph{scale invariant} if for every finite metric space $(X,d)$ and $\lambda > 0$ wex have
$$
\Cf(X,d) = \Cf(X, \lambda \cdot d),
$$
\ie, re-scaling the metric does not affect the clustering algorithm.
\end{definition}

\begin{definition}{Richness}{richness}
A clustering algorithm $\Cf$ is \emph{rich} if for every finite set $X$ and every partition $P$ of $X$ there exists a metric $d$ on $X$ such that $\Cf(X,d) = P$.
\end{definition}

\begin{definition}{Consistency}{}
Let $\Cf$ be a clustering algorithm. We say that it is \emph{consistent} if for every finite metric space $(X,d)$ and every metric $d'$ on $X$ such that
\begin{itemize}
    \item $d'(x,y) \leq d(x,y)$ if $x,y$ are in the same part of $\Cf(X,d)$;
    \item $d'(x,y) \geq d(x,y)$ if $x,y$ are in different parts of $\Cf(X,d)$;
\end{itemize}
we have $\Cf(X,d') = \Cf(X,d)$.
\end{definition}

Kleinberg showed that in combination these properties are impossible to satisfy.

\begin{theorem}{Kleinberg \cite[Thm.~2.1]{Kleinberg2002}}{kleinberg}
There exists no clustering algorithm that is scale invariant, rich and consistent at the same time.
\end{theorem}

Furthermore, if we drop any of the requirements we can find clustering algorithms satisfying the remaining two properties. As an example, $k$-means clustering from Definition \ref{def:kmeans_clustering} is clearly not rich since we restrict ourselves to partitions with $k$ or fewer\footnote{Depending on the implementation it is possible that $k$-means produces partitions with less than $k$ parts.} parts. It is, however, scale invariant.