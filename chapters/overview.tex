\chapter{Data Clustering}
\label{chapter__dataclustering}

Data clustering refers to the procedure of assigning to a set of data points some kind of ``grouping''. 
This is often used as a first step in machine learing pipelines as a way to reduce data complexity.
In the simplest case this ``grouping'' is \emph{partition} of the original datapoints, this is also called \emph{classical clustering} (see Figure \ref{fig:intro_partition} in the Introduction). Alternatively we could assign a so-called \emph{dendrogram}
to the datapoints, which is referred to as \emph{hierarchical clustering}.  In this context a dendrogram is a (rooted) binary tree where the leaves are the data points (Figure \ref{fig:intro_dendrogram}). 
In Section \ref{section__partitions} we define a more general notion of a dendrogram.

% \begin{figure}[h]
% \centering
% \begin{subfigure}[t]{0.45\textwidth}
%     \centering
%     \begin{tikzpicture}[sloped]
%         \node[circle,fill=black,inner sep=1pt] at (0, 2.3) {};
%         \node[circle,fill=black,inner sep=1pt] at (0.5, 1.6) {};
%         \node[circle,fill=black,inner sep=1pt] at (1.1, 2.1) {};

%         \node[circle,fill=black,inner sep=1pt] at (2.1, 1.2) {};
%         \node[circle,fill=black,inner sep=1pt] at (2.3, 1.9) {};

%         \node[circle,fill=black,inner sep=1pt] at (0.8, 0.3) {};
%         \node[circle,fill=black,inner sep=1pt] at (1.6, 0.2) {};
        
%         \draw[dashed,fill=myblue,fill opacity=0.15,rotate=40] (1.7,1.2) ellipse (1.1 and 0.8);
%         \draw[dashed,fill=mygreen,fill opacity=0.15,rotate=-12] (1.8, 1.9) ellipse (0.4 and 0.8);
%         \draw[dashed,fill=myred,fill opacity=0.15,rotate=-5] (1.2,0.3) ellipse (0.9 and 0.5);

%     \end{tikzpicture}
%     \phantomcaption
%     \caption*{\figurename~\thefigure\thesubfigure: A partition of seven points.}
%     \label{fig:dc_partition}
% \end{subfigure}
% \begin{subfigure}[t]{0.45\textwidth}
%     \centering
%     \begin{tikzpicture}[sloped]
%         \node (a) at (0,0) {a};
%         \node (b) at (1,0) {b};
%         \node (c) at (2,0) {c};
%         \node (d) at (3,0) {d};
%         \node (e) at (4,0) {e};
        
%         \draw  (a) |- (0.5,1);
%         \draw  (b) |- (0.5,1);

%         \draw  (0.5,1) |- (1,1.5);
%         \draw  (c) |- (1,1.5);
    
%         \draw (d) |- (3.5,1);
%         \draw (e) |- (3.5,1);

%         \draw (3.5,1) |- (3,2);
%         \draw (1,1.5) |- (3,2);

%         \draw[dashed]  (2,2) -- (2,2.5);
%         \draw[->, >=stealth] (-0.5,0) -- node[above]{\small{scale}} (-0.5,3);
%     \end{tikzpicture}
%     \phantomcaption
%     \caption*{\figurename~\thefigure\thesubfigure: A dendrogram of five points.}
%     \label{fig:dc_dendrogram}
% \end{subfigure}
% \end{figure}

We discuss two examples of commonly studied clustering algorithms. Namely, one classical and one hierarchical clustering algorithm. By contextualizing these algorithms with computations we demonstrate how they can fall short in certain cases.
This part is based on the books \cite{Everitt2011} and \cite{Scitovski2021}.

Then we talk about how clustering algorithms can be seen as remaining invariant under certain transformations of its input.
As an example, there might be a mechanism introducing noise into the data. If we model this noise as some transformation of the ``ground truth'' we can then try to construct a clustering algorithm that is invariant under this transformation. In other words the output of our clustering algorithm would not depend on the noise. This idea will naturally lead to \textsc{Kleinberg}'s impossibility result \cite{Kleinberg2002}.

\Needspace{5.5cm}

\section{Examples of Clustering Algorithms}
Let us consider a few examples of clustering algorithms.

\subsection[$k$-means]{$\boldsymbol{k}$-means}
A common method of clustering is to find a partition of the data into $k$ parts that minimizes some objective function.
One such example is the \emph{$k$-means} clustering objective \cite[Sec.~3.1]{Scitovski2021}.

\begin{definition}{$k$-means Clustering}{kmeans_clustering}
Let $\{x_1, \dots, x_n\} \subset \R^d$. For $k \in \N$ \emph{$k$-means clustering} refers to finding a partition $C_1, \dots, C_k$ of $\{x_1, \dots, x_n\}$ such that
\begin{equation*}
\label{eq:kmean_optimization}
    \sum_{i = 1}^k \sum_{x \in C_i} \|x - \mu_i\|^2_2
\end{equation*}
is minimal, where $\mu_k := \frac{1}{|C_k|}\sum_{x \in C_k} x$ is the \emph{center} of $C_k$\footnote{In case $C_k = \emptyset$ we set $\mu_k = 0 \in \R^d$.}.
\end{definition}
To find such a partition we would have to check all possible partitions of $x_1, \dots, x_n$. This is computationally infeasible. In practice, we use approximate methods such as \textsc{Lloyd}'s algorithm, which is sometimes simply referred to as \emph{the $k$-means algorithm} \cite[Sec.~3.1.2]{Scitovski2021}.

\begin{definition}{Lloyd's Algorithm}{lloyds_algorithm}
Given $\{x_1, \dots, x_n\} \subset \R^d$ and some \emph{centers} $z_1, \dots, z_k \in \R^d$
Lloyd's algorithm consists of iteratively applying the following two steps:

\begin{enumerate}
    \item \textbf{Assignment Step:} Assign each point $x_i$ to the cluster $C_j$ such that
    $$
    j = \argmin_{1 \leq l \leq k} \|x_i - z_l\|^2_2.
    $$

    \item \textbf{Update Step:} Update the centers $z_1, \dots, z_k$ by setting
    $$
    z_j = \frac{1}{|C_j|}\sum_{x \in C_j} x. \quad \forall \quad j \in \{1, \dots, k\},
    $$
    where $z_j = 0$ if $C_j = \emptyset$. 
\end{enumerate}
\end{definition}

One can show that Lloyd's algorithm converges after finitely many steps \cite[Thm.~3.14]{Scitovski2021}.
We will see that the choice of starting centers $z_1, \dots, z_k$ can have a significant impact on the outcome of the algorithm.

\begin{example}{}{}
Consider the set $\{(0,0), (0,1), (2,0), (2,1)\} \subset \R^2$, $k = 2$ and two possible choices of starting centers:

\begin{enumerate}
    \item $z_1 = (0, 1/2)$ and $z_2 = (2, 1/2)$
    \item $z_1' = (1, 0)$ and $z_2' = (1, 1)$
\end{enumerate}

Consider the first case. In the first assignment step we assign $(0,0)$ and $(0,1)$ to the first cluster $C_1$ and similarly $(2,0)$ and $(2,1)$ to the second cluster $C_2$. In the update step we update the centers and get $z_1 = \frac{1}{2}\left[(0,0) + (0,1)\right] = (0,1/2)$. Similarly, we get $z_2 = (2,1/2)$. Notice that these are the same centers as we started with. Therefore, the algorithm converges after just one iteration.
\medskip

Repeating the same computations for the second case we notice that the algorithm will also converge after just one step to the original centers. This will result in the red and blue clusters as seen in Figure \ref{fig:kmeans_example}.

%\begin{enumerate}
%    \item $C_1 = \{(0,0), (0,1)\}$ and $C_2 = \{(2,0), (2,1)\}$
%    \item $C_1' = \{(0,0), (2,0)\}$ and $C_2' = \{(0,1), (2,1)\}$
%\end{enumerate}

\begin{center}
\begin{minipage}{\linewidth}
\centering
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
        \node[circle,fill=black,inner sep=1pt] at (0,0) {};
        \node[circle,fill=black,inner sep=1pt] at (0,1.5) {};
        \node[circle,fill=black,inner sep=1pt] at (3,0) {};
        \node[circle,fill=black,inner sep=1pt] at (3,1.5) {};
    
        \node[circle,fill=myblue,inner sep=1pt, label={[myblue]:\small$z_1$}] at (0,0.75) {};
        \node[circle,fill=myred,inner sep=1pt, label={[myred]:\small$z_2$}] at (3,0.75) {};
    
        \draw[ellipse,dashed,fill=myblue,fill opacity=0.15] (0,0.75) ellipse (0.75 and 1.5);
        \draw[ellipse,dashed,fill=myred,fill opacity=0.15] (3,0.75) ellipse (0.75 and 1.5);
    \end{tikzpicture}\\
    \begin{center}
         starting centers $(1)$
    \end{center}
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
        \node[circle,fill=black,inner sep=1pt] at (0,0) {};
        \node[circle,fill=black,inner sep=1pt] at (0,1.5) {};
        \node[circle,fill=black,inner sep=1pt] at (3,0) {};
        \node[circle,fill=black,inner sep=1pt] at (3,1.5) {};
    
        \node[circle,fill=myblue,inner sep=1pt, label={[myblue]:\small$z_1'$}] at (1.5,0) {};
        \node[circle,fill=myred,inner sep=1pt, label={[myred]:\small$z_2'$}] at (1.5,1.5) {};
    
        \draw[ellipse,dashed,fill=myblue,fill opacity=0.15] (1.5,0) ellipse (2 and 0.65);
        \draw[ellipse,dashed,fill=myred,fill opacity=0.15] (1.5,1.5) ellipse (2 and 0.65);
    \end{tikzpicture}\\
    \begin{center}
         starting centers $(2)$
    \end{center}
\end{minipage}
\captionof{figure}{$k$-means clustering for different starting centers.}
\label{fig:kmeans_example}
\end{minipage}
\end{center}

In fact the first solution is the one minimizing the $k$-means objective. So we see that the choice of initial centers is important for the outcome of the algorithm. For this reason in practice one might run the algorithm multiple times with different initial centers drawn from some distribution and then choose the best solution.
\end{example}

\Needspace{6cm}

\begin{example}{}{}
Another potential downside of the $k$-means algorithm is that its clusters are always convex. Consider the following data points in Figure \ref{fig:kemans_circles} for $k = 2$.

\begin{center}
\begin{minipage}{\linewidth}
\centering
\begin{tikzpicture}
    \draw[circle, dashed](0,0) circle (1);
    \draw[circle, dashed](0,0) circle (2);

    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0095579832468025,-0.23458616178292213) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8621356010904919,-0.43521673725850546) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8627616584531776,0.4128659891205529) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.8291421634685034,0.5054463113114217) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9764896823801532,0.12472092624358487) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0140626107241928,-0.13705972885468481) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.7797471398667898,0.7164817919382073) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5877999996419829,0.7418100674775248) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9897424809214725,-0.35865675131466895) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.0922792389809537,1.006191078288321) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.9935603794343448,0.11784757153395319) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.026524547115352,-0.37871897925085024) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9488139684656478,0.7747854617416964) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5164088657878309,2.03153994637684) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.429935616806283,1.5022872229353683) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.8365483721220037,1.9189627548136554) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.981301736481553,-0.14499075605710074) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6349515928709943,1.2202599759050379) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.5231409868905119,1.9793981704336547) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.053624362266639,1.6908381811818811) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.04231412307509439,2.045672752950578) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.06811082126040592,1.9438370464512291) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.0906354601250503,1.694220851300212) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.589017397924315,1.3208705982413913) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.2875236513311277,1.5206616702727054) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (0.6515674232388111,1.8678604281807427) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6558729051313483,1.1575803869149104) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.6685583816277019,1.1508647790179702) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9448141306887845,-0.28977280114742615) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9472636540198844,-0.28636654246930904) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.9295785207047487,0.48494891715335603) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.41340373800077374,1.97704090631188) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.416451128927296,1.3334319841181093) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (-0.5504121965152975,1.8450892982198965) {};
    \node[circle,fill=black,inner sep=1pt, color=myred, opacity=0.75] at (1.484969911606966,1.3723065854767136) {};
    % -----------------
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.914573789718945,-0.28964318073223894) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.1982533125570065,-0.8919178733968665) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.80577538812941,-0.607080998055582) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9412027083485228,-0.27269568749492573) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8148270419175926,0.4238878668815002) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6225375984421799,-0.8119486329925646) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8821678647126937,0.3648396878561235) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.7795857969200991,-0.6304341921488942) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7483950763114702,0.6729373613544269) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.2739384002725763,-1.0208542531185714) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9228380796524287,-0.16940862942773213) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9282962576231144,-0.42297153472927584) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.49750583441724194,-0.8706964828847926) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.16573977530367748,-0.9270432989794191) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.7154381602074034,-0.761747732857447) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.28063484315157317,-0.8794997600003289) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.0430965817766624,0.2575111200438646) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.20871850422394092,-1.0747146100323817) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6320684095077692,-0.7632679571994616) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9706230943148069,-0.13408763258353848) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9122031196423117,0.5420940005463377) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.15908376563698895,-1.0366970717997068) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9794313663308976,0.2768904673101241) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9410855486192096,-0.43158246153463864) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.8050509365055522,-0.7337938215702184) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7668888938180021,-0.6516232811273315) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.7569340532084752,-0.683852150730453) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.3874097599372178,-0.8483389110540501) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.2297367654792433,1.4958896410698033) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.8902258184052567,0.7894776893682616) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.6273208428361784,-1.8371947162534354) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9885817783568538,-1.7147623663359197) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.9386023582122706,-1.7236389913807584) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.3401106297221774,1.5451099391524472) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.7696595846944951,-0.835412527730738) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.94149620924287,0.7884962462992549) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.1613066182943586,-1.6658708071329087) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.9737616591333986,0.42415122258148036) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.0910291132157077,-1.6346094222929535) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.420125841706075,1.5367643867093461) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (0.8773373323041873,-1.83463023722728) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (1.0295543643648137,-1.6695857338970421) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.9628887251833387,0.3917097103265355) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-1.6082435557290202,-1.2449832245663626) {};
    \node[circle,fill=black,inner sep=1pt, color=myblue, opacity=0.75] at (-0.1367588700508612,-2.0242149878928153) {};

    \draw[dashed, color=myred, line width=1] (-1.9, 2.4) -- (2.2, -2); % diff = (4.1, -4.4)
    \draw[->, color=myred, >=stealth, line width=1] (-1.9 + 0.41, 2.4 - 0.44) -- (-1.9 + 0.41 + 0.22, 2.4 - 0.44 + 0.205);
    \draw[->, color=myred, >=stealth, line width=1] (2.2 - 0.41, -2 + 0.44) -- (2.2 - 0.41 + 0.22, -2 + 0.44 + 0.205);
\end{tikzpicture}
\captionof{figure}{Points generated by adding noise to two circles and the result of running the $k$-means algorithm for $k = 2$.}
\label{fig:kemans_circles}
\end{minipage}
\end{center}

In this case the $k$-means algorithm will fail to split the two circles as the clusters will always be convex and therefore split the data along a line.
\end{example}



\subsection{Linkage Clustering}
\label{section__linkage_clustering}
Let us now consider a clustering algorithm that will produce a dendrogram as an ouput. We will achieve this my consecutively merging clusters based on some distance criterion \cite[Sec.~4.2.2]{Everitt2011}.

\begin{definition}{Linkage Clustering }{linkage_clustering}
Let $X := \{x_1, \dots, x_n\} \subset \R^n$ and $\mathfrak{d}\colon\mathscr{P}(X) \times \mathscr{P}(X) \to \R_{\geq0}$ a \emph{distance function} between clusters. We start with the partition
$$
B^{(0)}_j := \{x_j\}$$
for $j \in \{1, \dots, n\}$. Given a partition
$$
B^{(k)}_1, \dots, B^{(k)}_{n - k} \quad \text{of} \quad x_1, \dots, x_n
$$
we successively merge the clusters $j$ and $\ell$ such that
$$
\mathfrak{d}(B^{(k)}_j,B^{(k)}_\ell)
$$
is minimal for $j,\ell \in \{1, \dots, n - k\}$.
\end{definition}
The reason this procedure produces a dendrogram is that every time we merge two clusters we can think of this as a new node in a binary tree (the dendrogram). So we started with $B^{(0)}_j = \{x_j\}$ as the leaves and ended with $X = B^{(n-1)}_1$ as the root.


Depending on the distance function $\mathfrak{d}$ we use we call \emph{linkage clustering}

\begin{itemize}
    \item \emph{single-linkage clustering} if $\mathfrak{d}_\mathrm{min} (B_i, B_j) := \min\{d(x,y): x \in B_i, y \in B_j\}$;
    \item \emph{complete-linkage clustering} if $\mathfrak{d}_\mathrm{max} (B_i, B_j) := \max\{d(x,y): x \in B_i, y \in B_j\}$;
    \item \emph{average-linkage clustering} if $\mathfrak{d}_\mathrm{avg}(B_i,B_j) := \frac{1}{|B_i||B_j|} \sum_{x \in B_i, y \in B_j} d(x,y)$.
\end{itemize}

\begin{example}{}{}
As an example, consider the data points in Figure \ref{fig:sl_clustering} and their corresponding dendrogram obtained from single-linkage clustering. Here we first merge the points $v,w$ and $z,y$ as they are the closest. Next we add the point $x$ to the cluster $\{v,w\}$ as $x$ is closer to $\{v,w\}$ than $\{y,z\}$. Finally, we merge the remaining two clusters $\{v,w,x\}$ and $\{y,z\}$ to obtain a dendrogram. Notice how this order is fully described on the righthand side.
\begin{center}
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node[circle,fill=black,inner sep=1pt, label={below:\small$v$}] (v) at (0, 0) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$w$}] (w) at (0, 1.5) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$x$}] (x) at (2.25, 0.75) {};
        \node[circle,fill=black,inner sep=1pt, label={below:\small$y$}] (y) at (6, 0) {};
        \node[circle,fill=black,inner sep=1pt, label={\small$z$}] (z) at (6, 1.5) {};

        \draw[dashed, color=mypurple, >=stealth] (v) -- (w) node[midway, above] {\tiny{$d(v,w) = 1$}};

        \draw[dashed, color=mypurple, >=stealth] (w) -- (x) node[midway, above] {\tiny{$d(w,x) = 2$}};
        \draw[dashed, color=mypurple, >=stealth] (v) -- (x) node[midway, below] {\tiny{$d(v,x) = 2$}};

        \draw[dashed, color=mypurple, >=stealth] (x) -- (z) node[midway, above] {\tiny{$d(x,z) = 3$}};
        \draw[dashed, color=mypurple, >=stealth] (x) -- (y) node[midway, below] {\tiny{$d(x,y) = 3$}};

        \draw[<->, dashed, color=mypurple, >=stealth] (z) -- (y) node[midway, above] {\tiny{$d(y,z) = 1$}};
    \end{tikzpicture}
\end{minipage}
\hfill %
\begin{minipage}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node (v) at (0,0) {v};
        \node (w) at (1,0) {w};
        \node (x) at (2,0) {x};
        \node (y) at (3,0) {y};
        \node (z) at (4,0) {z};
        
        \draw  (v) |- (0.5,1);
        \draw  (w) |- (0.5,1);

        \draw  (0.5,1) |- (1,1.5);
        \draw  (x) |- (1,1.5);
    
        \draw (y) |- (3.5,1);
        \draw (z) |- (3.5,1);

        \draw (3.5,1) |- (3,2);
        \draw (1,1.5) |- (3,2);

        \draw[dashed]  (2,2) -- (2,2.5);
    \end{tikzpicture}
\end{minipage}
\captionof{figure}{Example of single-linkage clustering with the data points $\{v,w,x,y,z\} \subset \R^2$ on the left and the resulting dendrogram on the right.}
\label{fig:sl_clustering}
\end{center}
\end{example}

A downside of this definition of linkage clustering is that the output will always be a binary tree. In particular, if we have three equidistant points it is unclear which should be merged first. We can fix this by introducing a \emph{tie-breaking} rule or relaxing the definition of a dendrogram to allow merging more than two clusters at a time. We will see more about this once 
we define \emph{hierarchical clustering functors} in Chapter \ref{chapter__clustering_functor}.

Some other problems with linkage clustering are that in the case of single-linkage clustering it has a lack of sensitivity for density, and it has a tendency to produce long chains as clusters. Complete-linkage and average-linkage clustering have nice properties in regard to density, but it was show that they are not stable under small perturbations of the data \cite[Sec.~3.6]{JMLR:v11:carlsson10a}, \cite{Lance1967-ci}.

\section{Invariance of Clustering Algorithms}
\label{seciton__preserving_structure}

A desireable property of a clustering algorithm might be that it remains invariant under certain transformations to the data.
The reason this is desirable is that we can model noise in the data as some transformation of the underlying ``ground truth''.
If our algorithm was invariant under these transformations we could hope to recover the ground truth. As we are going to see, it is often not realistic to expect that such an algorithm even exists. But this still motivates the description of clustering algorithms in terms of their invariances.

\begin{example}{}{}
Consider a finite set $X \subset \R^2$ representing people at a gathering. When searching for a clustering algorithm to detect friendship groups, it would make sense to only consider algorithms with the following invariances:
\begin{itemize}
    \item Translation Invariance;
    \item Rotation Invariance;
    \item Scale Invariance.
\end{itemize}
This means that for any $v \in \R^2$, $R \in \mathrm{SO}_2(\R)$ and $\lambda > 0$ we would like our clustering algorithm to produce the same partition on the data $v + X$, $R \circ X$ and $\lambda \cdot X$\footnote{These operations are understood to be elementwise and $\circ$ denotes the natural action of $\mathrm{SO}_2(\R)$ on $\R^2$.}. Here the intuition should be that such attributes are merely artifacts of the data collection process, in particular the choice of coordinate system.
\end{example}

\subsection{Kleinberg's Impossibility Theorem}
It is apparent that as we require more invariants from a clustering algorithm it becomes harder to construct one.
There exists an important result in this regard.
\textsc{Kleinberg} showed that for certain seemingly reasonable properties there exist no clustering algorithms that satisfy them \cite{Kleinberg2002}.
In the context of this theorem we define a clustering algorithm as follows\footnote{In Chapter \ref{chapter__clustering_functor} we will revisit the definition of a clustering algorithm, at which point we will call it a \emph{clustering functor}}:

\begin{definition}{Clustering Algorithm}{}
A \emph{clustering algorithm} $\Cf$ is a procedure that assigns to a finite metric space $(X,d)$ a partition of $X$ denoted by $\Cf(X,d)$.
\end{definition}


Here are the properties \textsc{Kleinberg} considered:

\begin{definition}{Scale Invariance}{}
We say that a clustering algorithm $\Cf$ is \emph{scale invariant} if for every finite metric space $(X,d)$ and $\lambda > 0$ we have
$$
\Cf(X,d) = \Cf(X, \lambda \cdot d),
$$
\ie, re-scaling the metric does not affect the clustering algorithm.
\end{definition}

\begin{definition}{Richness}{richness}
A clustering algorithm $\Cf$ is \emph{rich} if for every set $X$ and every partition $P$ of $X$ there exists a metric $d$ on $X$ such that $\Cf(X,d) = P$.
\end{definition}

\begin{definition}{Consistency}{}
Let $\Cf$ be a clustering algorithm. We say that it is \emph{consistent} if for every finite metric space $(X,d)$ and every metric $d'$ on $X$ such that
\begin{itemize}
    \item $d'(x,y) \leq d(x,y)$ if $x,y$ are in the same part of $\Cf(X,d)$;
    \item $d'(x,y) \geq d(x,y)$ if $x,y$ are in different parts of $\Cf(X,d)$.
\end{itemize}
we have $\Cf(X,d') = \Cf(X,d)$.
\end{definition}

Kleinberg showed that in combination these properties are impossible to satisfy.

\begin{theorem}{Kleinberg \cite[Thm.~2.1]{Kleinberg2002}}{kleinberg}
There exists no clustering algorithm that is scale invariant, rich and consistent at the same time.
\end{theorem}

Furthermore, if we drop any of the requirements we can find clustering algorithms satisfying the remaining two properties. As an example, $k$-means clustering from Definition \ref{def:kmeans_clustering} is clearly not rich since we restrict ourselves to partitions with $k$ or fewer\footnote{Depending on the implementation it is possible that $k$-means produces partitions with less than $k$ parts.} parts. It is, however, scale invariant.