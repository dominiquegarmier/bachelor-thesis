\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

A central problem in machine learning and data science is the task of clustering data. \Ie, finding patterns in data and grouping data points into clusters.
Underlying most clustering techniques is the notion of a distance between data points. Motivated by this, we think of \emph{data} as a finite metric space, \ie\ a finite set equipped with a metric.
And a clustering algorithm will be a map assigning to a finite metric space either a \emph{partition} (figure \ref{fig:intro_partition}) or a \emph{dendrogram} (figure \ref{fig:intro_dendrogram}) of the underlying set.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node[circle,fill=black,inner sep=1pt] at (0, 2.3) {};
        \node[circle,fill=black,inner sep=1pt] at (0.5, 1.6) {};
        \node[circle,fill=black,inner sep=1pt] at (1.1, 2.1) {};

        \node[circle,fill=black,inner sep=1pt] at (2.1, 1.2) {};
        \node[circle,fill=black,inner sep=1pt] at (2.3, 1.9) {};

        \node[circle,fill=black,inner sep=1pt] at (0.8, 0.3) {};
        \node[circle,fill=black,inner sep=1pt] at (1.6, 0.2) {};
        
        \draw[dashed,fill=myblue,fill opacity=0.15,rotate=40] (1.7,1.2) ellipse (1.1 and 0.8);
        \draw[dashed,fill=mygreen,fill opacity=0.15,rotate=-12] (1.8, 1.9) ellipse (0.4 and 0.8);
        \draw[dashed,fill=myred,fill opacity=0.15,rotate=-5] (1.2,0.3) ellipse (0.9 and 0.5);

    \end{tikzpicture}
    \phantomcaption
    \caption*{\figurename~\thefigure\thesubfigure: A partition of seven points.}
    \label{fig:intro_partition}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node (a) at (0,0) {a};
        \node (b) at (1,0) {b};
        \node (c) at (2,0) {c};
        \node (d) at (3,0) {d};
        \node (e) at (4,0) {e};
        
        \draw  (a) |- (0.5,1);
        \draw  (b) |- (0.5,1);

        \draw  (0.5,1) |- (1,1.5);
        \draw  (c) |- (1,1.5);
    
        \draw (d) |- (3.5,1);
        \draw (e) |- (3.5,1);

        \draw (3.5,1) |- (3,2);
        \draw (1,1.5) |- (3,2);

        \draw[dashed]  (2,2) -- (2,2.5);
        \draw[->, >=stealth] (-0.5,0) -- node[above]{\small{scale}} (-0.5,3);
    \end{tikzpicture}
    \phantomcaption
    \caption*{\figurename~\thefigure\thesubfigure: A dendrogram of five points.}
    \label{fig:intro_dendrogram}
\end{subfigure}
\end{figure}

A dendrogram is a (rooted) tree, where the leaves are the data points. Dendrograms are strictly more general than partitions, as at every \emph{scale} a dendrogram describes a partition of the data points.
Depending on the clustering algorithm a dendrogram will have to satisfy additional properties.

Clustering algorithms that produce a partition are sometimes called \emph{classical} and those that produce a dendrogram are called \emph{hierarchical}.
For classical clustering algorithms there exists an important impossibility result by \textsc{Kleinberg} \cite{Kleinberg2002}, which says that for a certain set of properties there exists no clustering algorithm satisfying them.
Hierarchical clustering algorithms allow for more complex structures to be captured. We will show that this additional capacity can be used in some sense to interpret \textsc{Kleinberg}'s impossibility result as a uniqueness result. This idea was previously discussed by \textsc{Carlsson} and \textsc{M\'emoli} \cite{JMLR:v11:carlsson10a}, \cite{Carlsson2010}. The result we present here are slightly more general.

To get to this result we will use the formalization of clustering algorithms described in \cite{Carlsson2010}. They used the language of category theory to succinctly describe properties of clustering algorithms. In particular, clustering algorithms are seen as special functors mapping from appropriate categories of finite metric spaces to partitions or dendrograms respectively. For this reason we refer to such clustering algorithms as \emph{classical} respectively \emph{hierarchical clustering functors}. We do not require the reader to be familiar with category theory and will introduce and give examples for all the necessary concepts.

We will define and discuss properties such as \emph{surjectivity}, \emph{splitting} and \emph{representability} of clustering functors and how they related to each other. A central object of study will be the so-called \emph{Vietoris-Rips} clustering functor which will have all these properties. Moreover, by interpreting \textsc{Kleinberg}'s conditions in the setting of hierarchical clustering functors as done in \cite[Sec.~7.3.1]{Carlsson2010}, we will see that the Vietoris-Rips functor is the only hierarchical clustering functor satisfying these conditions.

\section*{Overview of the Thesis}
\todo[this could be moved into the appendix?]
\begin{description}
    %\item[\Cref{chapter__preliminaries}.] This chapter is a refresher of some basic concepts and definitions. The reader is advised to refer back to this chapter if they are unfamiliar with certain notations.

    \item[\Cref{chapter__dataclustering}.] We discuss traditional methods of data clustering and explain the impossibility result by \textsc{Kleinberg} \cite{Kleinberg2002}.

    \item[\Cref{chapter__category_theory}.] In this chapter, we cover the necessary concepts from category theory and contextualize them with examples. For this chapter we follow \cite{Roman2017} and \cite{Leinster2014-dc}.

    \item[\Cref{chapter__clustering_functor}.] Here we define the notion of \emph{classical} and \emph{hierarchical clustering functors} by closely following the notation presented by \textsc{Carlsson} and \textsc{M\'emoli} \cite{Carlsson2010}.

    \item[\Cref{chapter__classical}.] We define and introduce properties of classical clustering functors. This is also the first time that we encounter the Vietoris-Rips clustering functor. As observed by \cite{JMLR:v11:carlsson10a}, \cite{Carlsson2010} this functor has some interesting properties and characterizations. It is also in this chapter where we show that the Vietoris-Rips is the unique \emph{regular} and \emph{surjective} $\gen$ classical clustering functor.

    \item[\Cref{chapter__hierarchical}.] By relating our findings from the previous chapter to the \emph{modified} \textsc{Kleinberg} \emph{conditions} presented by \cite[Sec.~7.3.1]{Carlsson2010}, we are finally able to show that the Vietoris-Rips functor is the only hierarchical clustering functor satisfying these conditions.
\end{description}

