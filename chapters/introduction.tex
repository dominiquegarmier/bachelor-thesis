\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

A central problem in machine learning and data science is the task of clustering data, \ie, ``grouping'' data points according to some patterns.
Underlying most clustering techniques is the notion of a distance between data points. Motivated by this, we think of \emph{data} as a finite metric space, \ie\ a finite set equipped with a metric and of a clustering algorithm as a map assigning to a finite metric space either a \emph{partition} (Figure \ref{fig:intro_partition}) or a \emph{dendrogram} (Figure \ref{fig:intro_dendrogram}) of the underlying set.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node[circle,fill=black,inner sep=1pt] at (0, 2.3) {};
        \node[circle,fill=black,inner sep=1pt] at (0.5, 1.6) {};
        \node[circle,fill=black,inner sep=1pt] at (1.1, 2.1) {};

        \node[circle,fill=black,inner sep=1pt] at (2.1, 1.2) {};
        \node[circle,fill=black,inner sep=1pt] at (2.3, 1.9) {};

        \node[circle,fill=black,inner sep=1pt] at (0.8, 0.3) {};
        \node[circle,fill=black,inner sep=1pt] at (1.6, 0.2) {};
        
        \draw[dashed,fill=myblue,fill opacity=0.15,rotate=40] (1.7,1.2) ellipse (1.1 and 0.8);
        \draw[dashed,fill=mygreen,fill opacity=0.15,rotate=-12] (1.8, 1.9) ellipse (0.4 and 0.8);
        \draw[dashed,fill=myred,fill opacity=0.15,rotate=-5] (1.2,0.3) ellipse (0.9 and 0.5);

    \end{tikzpicture}
    \phantomcaption
    \caption*{\figurename~\thefigure\thesubfigure: A partition of seven points.}
    \label{fig:intro_partition}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[sloped]
        \node (a) at (0,0) {a};
        \node (b) at (1,0) {b};
        \node (c) at (2,0) {c};
        \node (d) at (3,0) {d};
        \node (e) at (4,0) {e};
        
        \draw  (a) |- (0.5,1);
        \draw  (b) |- (0.5,1);

        \draw  (0.5,1) |- (1,1.5);
        \draw  (c) |- (1,1.5);
    
        \draw (d) |- (3.5,1);
        \draw (e) |- (3.5,1);

        \draw (3.5,1) |- (3,2);
        \draw (1,1.5) |- (3,2);

        \draw[dashed]  (2,2) -- (2,2.5);
        \draw[->, >=stealth] (-0.5,0) -- node[above]{\small{scale}} (-0.5,3);
    \end{tikzpicture}
    \phantomcaption
    \caption*{\figurename~\thefigure\thesubfigure: A dendrogram of five points.}
    \label{fig:intro_dendrogram}
\end{subfigure}
\end{figure}

A dendrogram is a (rooted) tree, where the leaves are the data points. Dendrograms are strictly more general than partitions, as at every \emph{scale} a dendrogram describes a partition of the data points.
Depending on the clustering algorithm a dendrogram has to satisfy additional properties.
Clustering algorithms that produce a partition are sometimes called \emph{classical} and those that produce a dendrogram are called \emph{hierarchical}.


Hierarchical clustering algorithms allow for more complex structures to be captured compared to partitions. We show that this additional capacity can be used in some sense to interpret \textsc{Kleinberg}'s impossibility result as a uniqueness result. This idea was previously discussed by \textsc{Carlsson} and \textsc{M\'emoli} \cite{JMLR:v11:carlsson10a}, \cite{Carlsson2010}. The result we present is slightly more general.

To get to this result we use a formalization of clustering algorithms described in \cite{Carlsson2010}. We use the language of category theory to succinctly describe properties of clustering algorithms. In particular, clustering algorithms are seen as special functors mapping from appropriate categories of finite metric spaces to partitions or dendrograms. For this reason, we refer to such clustering algorithms as \emph{classical} and \emph{hierarchical clustering functors} respectively. We do not require the reader to be familiar with category theory, and we introduce and give examples of all the necessary concepts.

First we work with classical clustering functors, where two central properites of clustering functors are considered.
\begin{itemize}
    \item \emph{Surjectivity}, which means that for any partition of a set of $n$ points we can find a metric on these $n$ points such that our clustering functor produces this partition.
    \item \emph{Splitting}, which refers to a particular behaviour of clustering functors on metric spaces with two points. Namely, two points are ``merged'' into a single cluster if and only if they are $\delta$-close for some $\delta > 0$.
\end{itemize}

The latter is a characterizing property of the so-called \emph{Vietoris-Rips} clustering functor \cite[Thm.~6.4]{Carlsson2010}.
In this thesis we show that under certain technical assumptions surjectivity implies splitting. In some sense this means that a global property of a clustering functor (surjectivity) implies a local property (splitting).

For classical clustering algorithms there exists an important impossibility theorem by \textsc{Kleinberg} \cite{Kleinberg2002} which states that there exists no clustering algorithm satisfying the following three properties at the same time:

\begin{itemize}
    \item \emph{Richness} is the term used by \textsc{Kleinberg} to describe surjectivity.
    \item  \emph{Consistency} is the property that decreasing the distance between points inside a cluster does not change the clustering.
    \item \emph{Scale invariance} refers to the fact that we can rescale the metric and the clustering remains the same.
\end{itemize}

At the end we consider a modified version of the above conditions presented in \cite[Sec.~7.3.1]{Carlsson2010} that concern hierarchical clustering functors, more precisely, \emph{scaling} hierarchical clustering functors. We show that scaling hierarchical clustering functors are essentially the same as classical clustering functors\footnote{We show that there is a one-to-one correspondence between scaling hierarchical clustering functors and what we call \emph{regular} classical clustering functors.}. Utilizing this trick and the tools described earlier, we can show that the modified \textsc{Kleinberg} conditions give rise to a unique (up to some transformation) hierarchical clustering functor.

% \section*{Overview of the Thesis}
% \todo[this could be moved into the appendix?]
% \begin{description}
%     %\item[\Cref{chapter__preliminaries}.] This chapter is a refresher of some basic concepts and definitions. The reader is advised to refer back to this chapter if they are unfamiliar with certain notations.

%     \item[\Cref{chapter__dataclustering}.] We discuss traditional methods of data clustering and explain the impossibility result by \textsc{Kleinberg} \cite{Kleinberg2002}.

%     \item[\Cref{chapter__category_theory}.] In this chapter, we cover the necessary concepts from category theory and contextualize them with examples. For this chapter we follow \cite{Roman2017} and \cite{Leinster2014-dc}.

%     \item[\Cref{chapter__clustering_functor}.] Here we define the notion of \emph{classical} and \emph{hierarchical clustering functors} by closely following the notation presented by \textsc{Carlsson} and \textsc{M\'emoli} \cite{Carlsson2010}.

%     \item[\Cref{chapter__classical}.] We define and introduce properties of classical clustering functors. This is also the first time that we encounter the Vietoris-Rips clustering functor. As observed by \cite{JMLR:v11:carlsson10a}, \cite{Carlsson2010} this functor has some interesting properties and characterizations.
%     % It is also in this chapter where we show that the Vietoris-Rips is the unique \emph{regular} and \emph{surjective} $\gen$ classical clustering functor.

%     \item[\Cref{chapter__hierarchical}.] By relating our findings from the previous chapter to the \emph{modified} \textsc{Kleinberg} \emph{conditions} presented by \cite[Sec.~7.3.1]{Carlsson2010}, we are finally able to show that the Vietoris-Rips functor is the only hierarchical clustering functor satisfying these conditions.
% \end{description}

