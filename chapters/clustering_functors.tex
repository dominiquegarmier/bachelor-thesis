\chapter{Clustering Functors}
\label{chapter__clustering_functor}

In this chapter we introduce the notion of a \emph{clustering functor}.
Ultimately, a clustering functor will be a functor, with certain properties, from a category of finite metric spaces to a category of partitions. Functoriality gives us a nice way of talking about certain invariances described in Section \ref{seciton__preserving_structure}.
This construction closely follows the one by \textsc{Carlsson} and \textsc{M\'emoli} \cite{Carlsson2010}.

\section{Finite Metric Spaces}
\label{section__finite_metric_spaces}

First we need to define the category of ``inputs'' of clustering algorithms. As we have seen before that it is natural to think of \emph{data} as a finite metric space. This motivates the following definition.

\begin{definition}{Finite Metric Spaces \cite[Sec.~3.2]{Carlsson2010}}{}
We define three categories $\iso, \inj$ and $\gen$, all sharing the same objects:
\begin{equation*}
\ob(\M) := \{(X,d): \text{$X$ a finite non-empty set and $d$ a metric on $X$}\}
\end{equation*}
for $\M \in \{\iso, \inj, \gen\}$.
The three categories are distinguished by their morphisms. For $A, B \in \ob(\M)$ we set:
\begin{itemize}
    \item $\mor_\gen(A,B)$ the distance non-increasing functions $f\colon A \to B$;
    \item $\mor_\inj(A,B)$ the distance non-increasing injective functions $f\colon A \to B$;
    \item $\mor_\iso(A,B)$ the isometries $f\colon A \to B$.
\end{itemize}
The composition of morphisms is given by the composition of functions, and the identity morphism is the identity function.
\end{definition}

Depending on what kind and the amount of ``structure'' we would like our clustering algorithms to ``preserve'', we can choose one of the three categories.
By construction, we have the inclusions
\begin{equation}
    \label{eq:inclusions_of_finite_metric_space_categories}
    \iso \subset \inj \subset \gen.
\end{equation}
Furthermore, this inclusion yields functors $\iso \to \inj \to \gen$ \cite[Def.~1.2.18]{Leinster2014-dc}.
As such, $\iso$ is the category with the fewest morphisms. Indeed, between most objects there are no morphisms.
All morphisms are between spaces of the same isometry class and have an inverse.
In contrast, $\gen$ has the most morphisms. In particular, for any metric spaces $A,B \in \ob(\gen)$ we have the morphism
$$
\mathrm{const}_b\colon A \to B, \ a \mapsto b
$$
for some $b \in B$.

\section{Partitions and Dendrograms}
\label{section__partitions}

The next step will be to define a suitable category for the ``outputs'' of clustering algorithms. Here we will define two options, one based on partitions (for classical algorithms), the other using dendrograms (for hierarchical algorithms).

However, we first need to introduce a bit of machinery for this.

% \begin{definition}{\todo[I think i should leave this out]}{relation}
%     Let $X$ be a set, a \emph{relation} on $X$ is a subset $R \subseteq X \times X$. And we write $x R y$ if $(x,y) \in R$.
%     A relation with all the following properties is called an \emph{equivalence relation}:
%     \begin{enumerate}
%         \item \emph{Reflexivity}: $\forall x \in X: x R x$.
%         \item \emph{Symmetry}: $\forall x,y \in X: x R y \implies y R x$.
%         \item \emph{Transitivity}: $\forall x,y,z \in X: x R y \text{ and } y R z \implies x R z$.
%     \end{enumerate}
%     In this case we will often use a symbol like $\sim$ instead of a letter like $R$ to denote the relation. The \emph{equivalence classes} of an equivalence relation $\sim$ on $X$ is the set of sets
%     $$
%     X/_{\sim} := \{[x]_\sim: x \in X\}
%     $$
%     where $[x]_\sim := \{y \in X: x \sim y\}$.
% \end{definition}

\begin{definition}{Partitions of Finite Sets}{}
Let $X$ be a finite set. A partition $P$ of $X$ is the set of equivalence classes $X/_{\sim_P}$ for some equivalence relation $\sim_P$ on $X$. The set of all such partitions is denoted by $\P(X)$.\par

\medskip Furthermore:
\begin{itemize}
    \item We interchangeably use $P$ (the partition) and $\sim_P$ (the corresponding equivalence relation).
    \item A single equivalence class $X_\alpha \in P$ will be called a \emph{block} or \emph{part} of $P$.
\end{itemize}
\end{definition}

It naturally makes sense to use partitions as outputs of clustering algorithms, where a block of a partition is to be interpreted as a \emph{cluster}.

At this point we need some additional structure that will allow us to define categories based on partitions.

\begin{definition}{}{}
Let $f\colon X \to Y$ be a map. For some partition $P \in \P(Y)$ we define $f^*(P) \in \P(X)$ to be the partition such that
\begin{equation*}
    \forall x,y \in X: x \sim_{f^*(P)} y \iff f(x) \sim_P f(y).
\end{equation*}
We call $f^*(P)$ the \emph{pullback partition of $P$ w.r.t. $f$}.
\end{definition}

\begin{example}{}{pullback}
Consider $X := \{a,b,c\}$, $Y := \{a,b\}$ and $f\colon X \to Y$ such that
$$
f(a) = a \quad \text{and} \quad f(b) = f(c) = b.
$$
If we have a partition
$$
Q := \{\{a\}, \{b\}\}
$$
of $Y$. Then $f^*(Q) = \{\{a\}, \{b,c\}\}$ is the pullback partition of $Q$ w.r.t $f$. Notice how $f^*(Q)$ consists of the preimages of the parts of $Q$. 
\end{example}

Additionally, we can define a partial order on $\P(X)$.

\begin{definition}{}{}
Let $P, Q \in \mathfrak{P}(X)$. Then we write $P \refines Q$ and say that $P$ \emph{refines} $Q$ if
\begin{equation*}
    \forall x,y \in X: x \sim_P y \implies x \sim_Q y.
\end{equation*}
This defines a partial order on $\P(X)$. Moreover, if $f \colon X \to Y$ is a map and $Q \in \P(Y)$ then we write $P \refines_f Q$ if $P \refines f^*(Q)$ and say that $P$ \emph{refines} $Q$ \emph{via} $f$.
\end{definition}

\begin{example}{}{}
Consider $X := \{a,b,c\}$ together with partitions
$$
P := \{\{a\}, \{b\}, \{c\}\} \quad \text{and} \quad Q := \{\{a,b\}, \{c\}\}.
$$
As the blocks in $P$ are always contained in some block of $Q$ we have $P \refines Q$.
\end{example}

\begin{example}{}{refinement_via}
Consider $X := \{a,b,c\}$ and $Y := \{a,b\}$ and $f\colon X \to Y$ as in Example \ref{exa:pullback}. If we have the partition 
$$
Q := \{\{a\}, \{b\}\}
$$
of $Y$ then we have already seen that $f^*(Q) = \{\{a\}, \{b,c\}\}$. Thus, for the partition
$$
P := \{\{a\}, \{b\}, \{c\}\}
$$
of $X$ we have $P \refines_f Q$.
\end{example}

A useful fact to remember is that if $\mathrm{id}\colon X \to X$ is the identity then
\begin{equation}
    \label{eq:refinement_via_identity}
    P \refines_{\mathrm{id}} Q \iff P \refines Q.
\end{equation}

When describing a partition it is often easier to describe a minimal set of conditions the partition relation must satisfy. More formally, we talk about taking the \emph{transitive closure} of a relation.

\begin{definition}{}{}
    Given a relation $\sim$ on a set $X$ its \emph{transitive closure} is the transitive relation $\sim^+$ such that for $x,y \in X$ we have
    $$
    x \sim^+ y
    $$
    if there exists a sequence $x = x_0, x_1, \dots, x_n = y$ with $x_i \sim_R x_{i+1}$ for all $i = 0, \dots, n-1$ \cite[p.337]{Lidl1997-kc}.
\end{definition}

In particular, if $\sim$ is reflexive and symmetric then $\sim^+$ is an equivalence relation.
Later we will use this fact to define equivalence relations as transitive closures of reflexive and symmetric relations.

\begin{example}{}{}
Let $G = (V,E)$ be a graph then $E$ is a binary symmetric relation on $V$. Two points $v,w \in V$ lie in the same connected component of $G$ if there exists a path, \ie, a sequence $v =v_0, v_1, \dots, v_n = w$ such that $(v_i, v_{i+1}) \in E$ for all $i = 0, \dots, n-1$. The resulting partition of $V$ into connected components corresponds to the transitive closure of $E$.
\end{example}

\begin{definition}{}{}
We say that $P \in \P(X)$ is:
\begin{enumerate}
    \item \emph{discrete} if $x \sim_P y \iff x = y$.
    \item \emph{trivial} if $x \sim_P y$ for all $x,y \in X$.
\end{enumerate}
\end{definition}

Naturally, if $Q$ is discrete and $P$ is trivial then $Q \refines R \refines P$ for all $R \in \P(X)$.

Like in Chapter \ref{chapter__dataclustering} we also want to consider hierarchical clustering.
For this we introduce the notion of a \emph{dendrogram}.

\begin{definition}{Dendrogram \cite[Def.~2.2]{Carlsson2010}}{dendrogram}
A map $\theta: \R_{\geq0} \to \mathfrak{P}(X)$ with
\begin{enumerate}
    \item $\forall r,s \in \R_{\geq0}: r \leq s \implies  \theta(r) \refines \theta(s)$,
    \item $\exists r,s \in \R_{\geq0}$ such that $\theta(r)$ is trivial and $\theta(s)$ is discrete\footnote{In particular, we get that $\theta(0)$ is always discrete.},
    \item $\forall r \in \R_{\geq0} \,\exists \varepsilon > 0$ such that $\theta$ is constant on $[r, r + \varepsilon)$,
\end{enumerate}
is called a \emph{dendrogram} of $X$. Sometimes $r$ is referred to as the \emph{scale}.
\end{definition}

This will be familiar to anyone who has seen \emph{persistent homology} where such properties are referred to as \emph{persistence} \cite[Chap.~3]{Carlsson2014}.

As for the relevance of the last condition, consider the following remark.

\begin{myremark}{}{regularity_of_dendrograms}
Since the metric spaces we are interested in are finite, we could decide to omit the third condition in the definition of a dendrogram.
Dendrograms would still have discrete scale \ie\ the dendrogram would still be constant on intervals.
However, it would not be clear what value the dendrogram would take at the endpoints of these intervals.
This technicality will become important for some uniqueness theorems we present later.
\end{myremark}

\begin{example}{}{}
As an example of a dendrogram consider Figure \ref{fig:dendrogram_example}.
\begin{center}
\begin{minipage}{\linewidth}
\centering
\begin{tikzpicture}
    \node (a) at (0,-0.25) {a};
    \node (b) at (1,-0.25) {b};
    \node (c) at (2,-0.25) {c};
    \node (d) at (3,-0.25) {d};
    
    \draw  (a) |- (0.5,0.5);
    \draw  (b) |- (0.5,0.5);

    \draw  (0.5,0.5) |- (1,1.25);
    \draw  (c) |- (1,1.25);


    \draw  (1,1.25) |- (1.5,2);
    \draw (d) |- (1.5,2);

    \draw[dashed]  (1.5,2) -- (1.5,2.5);
    \draw[->, >=stealth] (-0.5,-0.25) -- (-0.5,2.5);

    \draw[dashed] (-0.25, 0.6) -- (3.25, 0.6);
    \draw[color=myred, opacity=0.5, line width=1mm] (-0.2, 0.6) -- (1.2, 0.6);
    \draw[color=myblue, opacity=0.5, line width=1mm] (1.8, 0.6) -- (2.2, 0.6);
    \draw[color=mygreen, opacity=0.5, line width=1mm] (2.8, 0.6) -- (3.2, 0.6);

    \draw[dashed] (-0.25, 1.9) -- (3.25, 1.9);
    \draw[color=mypurple, opacity=0.5, line width=1mm] (-0.2, 1.9) -- (2.2, 1.9);
    \draw[color=mygreen, opacity=0.5, line width=1mm] (2.8, 1.9) -- (3.2, 1.9);

    \draw[] (-0.55, 0.6) -- node[left]{$1$} (-0.45, 0.6);
    \draw[] (-0.55, 1.9) -- node[left]{$2$} (-0.45, 1.9);

    \draw[|->, >=to] (3.35, 0.6) -- (4.25, 0.6);
    \draw[|->, >=to] (3.35, 1.9) -- node[above]{$\theta$} (4.25, 1.9);

    \node[circle,fill=black,inner sep=1pt] (z) at (5, 0.6) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (6, 0.6) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (7, 0.6) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (8, 0.6) {};

    \draw[ellipse, dashed, fill=myred, fill opacity=0.15] (5.5, 0.6) ellipse (0.75 and 0.25);
    \draw[ellipse, dashed, fill=myblue, fill opacity=0.15] (7, 0.6) ellipse (0.25 and 0.25);
    \draw[ellipse, dashed, fill=mygreen, fill opacity=0.15] (8, 0.6) ellipse (0.25 and 0.25);

    \node[circle,fill=black,inner sep=1pt] (z) at (5, 1.9) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (6, 1.9) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (7, 1.9) {};
    \node[circle,fill=black,inner sep=1pt] (z) at (8, 1.9) {};

    \draw[ellipse, dashed, fill=mypurple, fill opacity=0.15] (6, 1.9) ellipse (1.25 and 0.25);
    \draw[ellipse, dashed, fill=mygreen, fill opacity=0.15] (8, 1.9) ellipse (0.25 and 0.25);

    \node (a) at (5,-0.25) {a};
    \node (b) at (6,-0.25) {b};
    \node (c) at (7,-0.25) {c};
    \node (d) at (8,-0.25) {d};

        \node[rotate=90] at (6.5, 1.25) {$\preceq$};
\end{tikzpicture}
\captionof{figure}{A dendrogram $\theta: \R_{\geq0} \to \P(X)$ with the four points $X = \{a,b,c,d\}$.}
\label{fig:dendrogram_example}
\end{minipage}
\end{center}

Notice how in this case the monotonicity condition is satisfied \eg\ $\theta(1) \refines \theta(2)$ as shown on the righthand side. The second condition of a dendrogram is also met as it becomes trivial at the top and discrete at the bottom. Finally, the third condition tells us what value the dendrogram takes whenever a ``merge happens''.
\end{example}

We can finally define the categories which will be the ``outputs'' of clustering functors.

\begin{definition}{Classical Clustering Outputs\cite[Def.~3.2]{Carlsson2010}}{output_classical_clustering_functors}
The category $\C$ of \emph{classical clustering outputs} is defined by
\begin{equation*}
    \ob(\C) := \{(X, P): X \text{ finite non-empty and } P \in \P(X)\}
\end{equation*}
and for all $(X,P), (Y,Q) \in \ob(\C)$ we have the morphisms
\begin{equation*}
    \mor_\C((X,P), (Y,Q)) := \{f\colon X \to Y: P \refines_f Q\}.
\end{equation*}
In short, we write $(X,P) \refines_f (Y,Q)$ for such a morphism. As before, the composition is given by composition of maps and the identity is the identity map.
\end{definition}

\begin{definition}{Hierarchical Clustering Outputs\cite[Def.~3.3]{Carlsson2010}}{output_hierarchical_clustering_functors}
Similarly, we define the category $\H$ of \emph{hierarchical clustering outputs}, given by
\begin{equation*}
\begin{multlined}[c][.8\displaywidth]
    \ob(\H) := \Big\{(X, \theta_X): X \text{ finite non-empty} \\ \text{and } \theta_X: \R_{\geq0} \to \P(X) \ \text{a dendrogram}\Big\}
\end{multlined}
\end{equation*}
and for all $(X, \theta_X), (Y, \theta_Y) \in \ob(\H)$ we have the morphisms
\begin{equation*}
    \mor_\H((X, \theta_X), (Y,\theta_Y)) :=
    \left\{f\colon X \to Y: \forall r \in \R_{\geq0}: \theta_X(r) \refines_f \theta_Y(r)) \right\}.
\end{equation*}
Again, we write $(X, \theta_X) \refines_f (Y, \theta_Y)$ for such a morphism, and the composition and identity are defined as before.
\end{definition}

Another way of thinking about morphisms in $\C$ (or $\H$) is that given ${(X,P), (Y,Q) \in \ob(\C)}$ a morphism $f \in \mor_\C((X,P), (Y,Q))$ is simply a map $f\colon X \to Y$ such that
$$
\forall x,y \in X: x \sim_P y \implies f(x) \sim_Q f(y).
$$

\section{Clustering Functors}
We now have all the tools to define a \emph{clustering functor}.

\begin{definition}{Clustering Functor \cite[Sec.~4.1]{Carlsson2010}}{}
Let $\M \in \{\iso, \inj, \gen\}$ and $\A \in \{\C, \H\}$. An $\M$-\emph{functorial clustering functor} (or $\M$ \emph{clustering functor}) is a functor from $\M$ to $\A$
$$\Cf : \M \longrightarrow \A$$
such that $\Cf$ factorizes the forgetful functors (see \ref{not:factorizing_forgetful_functor}). If $\A = \C$ we say that $\Cf$ is \emph{classical} and otherwise if $\A = \H$ we say it is \emph{hierarchical}.
\end{definition}

We can express the functoriality of a clustering functor $\Cf$ by the following commutative diagram.
\begin{equation*}
    \begin{tikzcd}
    {(X,d)} \arrow[r, "f"] \arrow[d, "\Cf", Rightarrow] & {(Y,d)} \arrow[d, "\Cf", Rightarrow] \\
    {\Cf(X,d)} \arrow[r, "\refines_f"]                  & {\Cf(Y,d)}
    \end{tikzcd}
\end{equation*}

For a hierarchical clustering functor $\Hf$ we will use the simplified notation:
$$
\Hf(X,d;r) := (X,\theta_X(r)).
$$
Moreover, for any $r \in \R_{\geq0}$ a hierarchical clustering functor $\Hf$ induces a classical clustering functor $\Hf(\ \cdot \ ;r)$.

\begin{myremark}{}{induced_functor_by_inclusion}
Recall the inclusions \eqref{eq:inclusions_of_finite_metric_space_categories} and their induced functors.
Given a clustering functor on a larger category, say $\Cf\colon \gen \to \C$, this immediately induces clustering functors on the smaller categories by pre-composition with the inclusion functors $\iso \to \inj \to \gen$. We will use the same symbol for the induced functors.
\end{myremark}

With this in mind, it makes sense to think of the categories $\iso, \inj, \gen$ as being different levels of ``structure'' a clustering functor can ``preserve'' where $\iso$ is the least restrictive and $\gen$ the most restrictive.


\begin{myremark}{}{}
We can extend the partial order $\refines$ on $\P(X)$ to a partial order on clustering functors.
In particular, if $\Cf, \mathfrak{D}\colon \M \to \C$ are classical clustering functors we write $\Cf \refines \mathfrak{D}$ if
\begin{equation*}
    \forall (X,d) \in \ob(\M): \Cf(X,d) \refines \mathfrak{D}(X,d).
\end{equation*}
And in case of hierarchical clustering functors $\Cf, \mathfrak{D}\colon \M \to \H$ we write $\Cf \refines \mathfrak{D}$ if
\begin{equation*}
    \forall (X,d) \in \ob(\M) \, \forall r \in \R_{\geq0}: \Cf(X,d;r) \refines \mathfrak{D}(X,d;r).
\end{equation*}
\end{myremark}

\subsection{The Vietoris-Rips Clustering Functor}

A very natural example of a clustering functor is the so-called \emph{Vietoris-Rips clustering functor}. It can be interpreted as both a classical and hierarchical clustering functor. The next two chapters will be dedicated to studying the unique properties of this clustering functor.

\begin{example}{Vietoris-Rips Functor \cite[Def.~6.1]{Carlsson2010}}{classical_vr}
    Let $\delta > 0$ and $\M \in \{\iso, \inj, \gen\}$. The \emph{classical Vietoris-Rips clustering functor}
    $$
    \Rf_\delta\colon \M \to \C
    $$
    assigns to each metric space $(X,d) \in \M$ the partition $(X,P)$ where $\sim_P$ is the equivalence relation generated by:
    \begin{equation}
        \label{eq:vietoris_rips_equivalence_relation}
        \forall x,y \in X: d(x,y) \leq \delta \implies x \sim_P y.
    \end{equation}

    Let us now show that $\Rf_\delta$ is indeed a clustering functor.
    For this, it is sufficient to show that $\Rf_\delta$ is $\gen$-functorial. By Remark \ref{rem:induced_functor_by_inclusion}, functoriality over $\iso$ and $\inj$ will follow.
    \medskip

    Let $(X,d),(Y,d') \in \ob(\gen)$ and $(X,P) := \Rf_\delta(X)$ as well as $(Y,Q) := \Rf_\delta(Y)$.
    Take any $f \in \mor_\gen(X,Y)$, recall that $f$ is distance non-increasing.
    We have to show that 
    $$P \refines_f Q.$$
    Indeed, let $x,y \in X$ such that $d(x,y) \leq \delta$.
    Then, we have
    $$
    d'(f(x), f(y)) \leq d(x,y) \leq \delta
    $$
    and therefore $f(x) \sim_{Q} f(y)$.
    By taking the transitive closure we get that $P \refines_f Q$, and we are done.
\end{example}

\begin{myremark}{}{}
One could use a proper inequality in \eqref{eq:vietoris_rips_equivalence_relation}. The entire theory we are going to present would still hold, provided we tweak certain definitions accordingly, in particular, Definition \ref{def:dendrogram}.
\end{myremark}

By varying the parameter $\delta$ we can obtain the hierarchical Vietoris-Rips clustering functor.

\begin{example}{Vietoris-Rips Functor \cite[Ex.~7.1]{Carlsson2010}}{hierarchical_vr}
For $\M \in \{\gen,\inj,\iso\}$ we can define the \emph{hierarchical Vietoris-Rips clustering functor} by
$$
\Rf\colon \M \to \H
$$
where for $(X,d) \in \ob(\M)$ and $\delta > 0$ we set
$$
\Rf(X,d; \delta) := \Rf_\delta(X,d).
$$
Notice that since $\Rf_\delta$ is a classical clustering functor and $\Rf_\delta \refines \Rf_{\delta'}$ for $\delta \leq \delta'$, $\Rf$ is indeed a hierarchical clustering functor.
\end{example}

To get an understanding for how the Vietoris-Rips functor works, consider the following concrete example.

\begin{example}{}{}
Consider the seven points $\{a,b,c,d,e,f,g\} \subset \R^2$ shown in the Figure \ref{fig:vietoris_rips_example}. $\Rf_\delta$ creates the clusters $\{a,b,c\}$ and $\{d,e,f,g\}$, drawn in red and blue.
\begin{center}
\begin{minipage}{\textwidth}
\centering
\begin{tikzpicture}[sloped]
    \draw[dashed,fill=mygreen,fill opacity=0.1](2.3, 4.8) node[circle,fill=black,inner sep=1pt,fill opacity=1]{} circle (1);
    \draw[dashed,fill=mygreen,fill opacity=0.1](3.2, 4.0) node[circle,fill=black,inner sep=1pt,fill opacity=1]{} circle (1);
    \draw[dashed,fill=mygreen,fill opacity=0.1](1.7, 3.4) node[circle,fill=black,inner sep=1pt,fill opacity=1]{} circle (1);

    \draw[dashed,fill=mygreen,fill opacity=0.1](0.7, 1.3) node[circle,fill=black,inner sep=1pt,fill opacity=1]{} circle (1);
    \draw[dashed,fill=mygreen,fill opacity=0.1](3.9, 1.3) node[circle,fill=black,inner sep=1pt,fill opacity=1]{} circle (1);
    \draw[dashed,fill=mygreen,fill opacity=0.1](2.1, 1.1) node[circle,fill=black,inner sep=1pt,fill opacity=1]{} circle (1);
    \draw[dashed,fill=mygreen,fill opacity=0.1](3.4, 0.3) node[circle,fill=black,inner sep=1pt,fill opacity=1]{} circle (1);

    \draw[<->, >=stealth, color=mypurple, thick, opacity=0.6] (2.3, 4.8) -- (2.3 - 0.985, 4.8 + 0.174) node[midway, above]{\tiny{$\delta / 2$}};
    %\draw[<->, >=stealth, color=mypurple, thick, opacity=0.4] (3.2, 4.0) -- (3.2 + 0.866, 4.0 - 0.5) node[midway, above]{\tiny{$\delta$}};
    %\draw[<->, >=stealth, color=mypurple, thick, opacity=0.4] (1.7, 3.4) -- (1.7 - 0.174, 3.4 - 0.985) node[midway, above]{\tiny{$\delta$}};

    %\draw[<->, >=stealth, color=mypurple, thick, opacity=0.4] (0.7, 1.3) -- (0.7 - 0.5, 1.3 + 0.866) node[midway, above]{\tiny{$\delta$}};
    %\draw[<->, >=stealth, color=mypurple, thick, opacity=0.4] (3.9, 1.3) -- (3.9 + 0.866, 1.3 + 0.5) node[midway, above]{\tiny{$\delta$}};
    %\draw[<->, >=stealth, color=mypurple, thick, opacity=0.4] (2.1, 1.1) -- (2.1 - 0, 1.1 - 1) node[midway, above]{\tiny{$\delta$}};
    %\draw[<->, >=stealth, color=mypurple, thick, opacity=0.4] (3.4, 0.3) -- (3.4 + 0.866, 0.3 - 0.5) node[midway, above]{\tiny{$\delta$}};

    \draw[myred!50!black, thick, opacity=0.9, fill=myred, fill opacity=0.25] (2.3, 4.8) -- (3.2, 4.0) -- (1.7, 3.4) -- cycle;
    \draw[myblue!50!black, thick, opacity=0.9, fill=myblue, fill opacity=0.25] (3.9, 1.3) -- (2.1, 1.1) -- (3.4, 0.3) -- cycle;
    \draw[myblue!50!black, thick, opacity=0.9] (0.7, 1.3) -- (2.1, 1.1);

    \node[above] at (2.3, 4.8) {$a$};
    \node[above] at (3.2, 4.0) {$b$};
    \node[left] at (1.7, 3.4) {$c$};

    \node[left] at (0.7, 1.3) {$d$};
    \node[right] at (3.9, 1.3) {$e$};
    \node[above] at (2.1, 1.1) {$f$};
    \node[below] at (3.4, 0.3) {$g$};
\end{tikzpicture}
\captionof{figure}{Clusters (red and blue) produced by the Vietoris-Rips functor $\Rf_\delta$ for on the points $\{a,b,c,d,e,f,g\} \subset \R^2$.}
\label{fig:vietoris_rips_example}
\end{minipage}
\end{center}
For clarity, we draw the circles with radius $\delta / 2$, \ie\ two points are in the same cluster if circles around them with radius $\delta / 2$ intersect.
\end{example}

% \begin{example}{\cite[Ex.~6.1]{Carlsson2010}}{}
% Given $(X,d) \in \ob(\inj)$ we define
% $$
% \delta_{(X,d)} := \begin{cases}
%     \hfil 1 & \text{if $|X| = 1$} \\
%     \left(\min\{d(x,y): x \neq y \text{ in } X\}\right)^{-1} & \text{else}
% \end{cases}.
% $$
% Consider now the equivalence relation $\sim_{(X,d)}$ of $X$ generated by:
% $$
% \forall x,y \in X: d(x,y) \leq \delta_{(X,d)} \implies x \sim_{(X,d)} y.
% $$
% Let $P_{(X,d)} \in \P(X)$ denote the corresponding partition of $X$.
% The resulting map $\Cf: \inj \to \C$ such that $\Cf(X,d) := (X, P_{(X,d)})$ is a classical clustering functor.

% \medskip
% Let us check functoriality for $f \in \mor_\inj((X,d),(Y,d'))$. Since $f$ is distance non-increasing we immediately have that $\delta_{(X,d)} \leq \delta_{(Y,d')}$. Take $x,y \in X$ with
% \begin{equation}
%     d(x,y) \leq \delta_{(X,d)}.
%     \label{eq:distance_condition_example}
% \end{equation}
% Again, since $f$ is distance non-increasing, we get that
% $$
% d'(f(x),f(y)) \leq d(x,y) \leq \delta_{(X,d)} \leq \delta_{(Y,d')}
% $$
% and therefore $f(x) \sim_{(Y,d')} f(y)$. By taking the transitive closure over the condition in \eqref{eq:distance_condition_example}, we get that $P_{(X,d)} \refines_f P_{(Y,d')}$.
% This shows that $\Cf$ is indeed a clustering functor.
% \end{example}

% \begin{example}{}{}
% Let $\M \in \{\iso, \inj, \gen\}$
% Consider a classical clustering functor $\Cf: \M \to \C$ and $R > 0$.
% Then, for a metric space ${(X,d) \in \ob(\M)}$ let $\theta_X: \R_{\geq0} \to \P(X)$ be the dendrogram given by
% $$
% \theta_X(r) :=
% \begin{cases}
%     \text{discrete partition of $X$}& \text{if $r = 0$} \\
%     \hfil \text{trivial partition of $X$}& \text{if $r \geq R$} \\
%     \hfil \Cf(X,d) & \text{else}
% \end{cases}
% $$
% for all $r \in \R_{\geq0}$. 
% This defines a hierarchical clustering functor $\Hf: \M \to \H$ by setting
% $$
% \Hf(X,d) := (X,\theta_X)
% $$
% for all $(X,d) \in \ob(\M)$.
% \end{example}
